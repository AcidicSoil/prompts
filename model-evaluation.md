# Model Evaluation

Trigger: /model-evaluation

Purpose: Try a new model and compare outputs against a baseline.

## Steps

1. Define a benchmark set from recent tasks.
2. Run candidates and collect outputs and metrics.
3. Analyze failures and summarize where each model excels.

## Output format

- Summary table and recommendations to adopt or not.
