{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Server Bootstrap",
        "description": "Initialize a Node.js TypeScript project and set up the basic MCP server using stdio transport. This includes creating the main entry point, configuring graceful shutdown, and establishing structured NDJSON logging.",
        "details": "Create a `package.json` with dependencies like `@modelcontextprotocol/sdk` and `typescript`. Set up `tsconfig.json` for compilation to `dist/`. Implement the main server file (`src/index.ts`) to instantiate `StdioServerTransport`, register server info (name, version), and handle SIGINT/SIGTERM for graceful shutdown. Implement a basic structured logger that outputs NDJSON.",
        "testStrategy": "Manually start the server and connect with an MCP client like MCP Inspector. Verify that the server reports its name and version. Send a SIGINT signal and confirm the server logs a 'server_stop' message and exits cleanly.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure package.json and tsconfig.json",
            "description": "Update the project's package.json to include necessary dependencies and scripts. Configure tsconfig.json to define the TypeScript compilation settings, ensuring output is directed to the dist/ directory.",
            "dependencies": [],
            "details": "In `package.json`, add `@modelcontextprotocol/sdk` to `dependencies`. Add `typescript`, `ts-node`, and `@types/node` to `devDependencies`. Add a `build` script (`\"tsc\"`) and a `start` script (`\"node dist/index.js\"`). In `tsconfig.json`, set `compilerOptions.outDir` to `./dist`, `rootDir` to `./src`, and ensure `moduleResolution` is `node`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement a Structured NDJSON Logger",
            "description": "Create a simple logger in `src/logger.ts` that writes structured log messages to `stdout` in NDJSON (Newline Delimited JSON) format. This utility will be used for all server logging.",
            "dependencies": [],
            "details": "Implement a `Logger` class or object in `src/logger.ts` with `info`, `warn`, and `error` methods. Each method should accept a message and an optional metadata object. The output for each log entry must be a single-line JSON string containing a timestamp, log level, message, and any metadata, written to `process.stdout`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Bootstrap MCP Server with Stdio Transport",
            "description": "In the main entry point `src/index.ts`, initialize the core server components by instantiating the MCP server and the standard I/O transport layer.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "In `src/index.ts`, import the logger from `./logger.ts`. Import `MCPServer` and `StdioServerTransport` from `@modelcontextprotocol/sdk`. Inside a `main` async function, instantiate the logger, then the `StdioServerTransport`, and finally the `MCPServer`, passing the transport and logger to its constructor.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Register Server Information and Start Listening",
            "description": "Configure the server with its identity by registering its name and version, and then start the server to begin listening for client connections over stdio.",
            "dependencies": [
              "1.3"
            ],
            "details": "In the `main` function of `src/index.ts`, read the `version` from `package.json`. Call the `server.info.register()` method with a server name (e.g., \"MCP Reference Server\") and the version. After registration, call `server.start()` and log a confirmation message indicating the server is running.",
            "status": "done",
            "testStrategy": "After completing this subtask, run the `start` script. Use an MCP client like MCP Inspector to connect to the running process and verify that the server's name and version are correctly reported."
          },
          {
            "id": 5,
            "title": "Implement Graceful Shutdown and Top-Level Error Handling",
            "description": "Add signal handlers in `src/index.ts` to ensure the server shuts down cleanly on SIGINT/SIGTERM signals and that any uncaught exceptions are logged before exiting.",
            "dependencies": [
              "1.3"
            ],
            "details": "Add listeners for `process.on('SIGINT', ...)` and `process.on('SIGTERM', ...)`. The handler should invoke `await server.stop()`, log a 'server_stop' message, and exit with `process.exit(0)`. Also, implement a `process.on('uncaughtException', ...)` handler to log the fatal error using the NDJSON logger before exiting with a non-zero status code.",
            "status": "done",
            "testStrategy": "With the server running, send a SIGINT signal (Ctrl+C). Verify that the 'server_stop' message is logged in NDJSON format and the process exits cleanly without an error code."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Safety Control Utilities",
        "description": "Create utility functions for redacting secrets in logs and capping payload sizes to prevent data leakage and excessive resource usage.",
        "details": "Create a logging utility that wraps the base logger. This utility should scan log objects for keys matching the regex `/(key|secret|token)/i` and replace their values with `[redacted]`. Create a `capPayload` function that truncates strings larger than ~1 MB and appends a note like `[truncated N bytes]`. These utilities should be pure functions and easily testable.",
        "testStrategy": "Unit test the redaction logic by passing objects with keys like `API_KEY` and `SECRET_TOKEN`. Unit test the `capPayload` function with strings smaller than, equal to, and larger than the 1 MB threshold to verify correct truncation and messaging.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `redactSecrets` Utility Function",
            "description": "Implement a pure function to recursively scan an object and redact values for keys matching a specific regex.",
            "dependencies": [],
            "details": "In a new file, `src/utils/safety.ts`, create and export a pure function `redactSecrets(data: any)`. This function should recursively traverse any given object or array. If it encounters an object key that matches the case-insensitive regex `/(key|secret|token)/i`, it must replace the corresponding value with the string `[redacted]`. The function should handle nested objects and arrays without modifying the original input object (i.e., it should return a new, deep-cloned object).",
            "status": "done",
            "testStrategy": "This function will be tested in a subsequent subtask. Focus on a clean, recursive implementation."
          },
          {
            "id": 2,
            "title": "Create `capPayload` Utility Function",
            "description": "Implement a pure function to truncate large strings to a specified maximum size.",
            "dependencies": [],
            "details": "In the same `src/utils/safety.ts` file, create and export a pure function `capPayload(payload: string, maxSize: number = 1024 * 1024)`. This function will check if the input string's size exceeds `maxSize`. If it does, the function should truncate the string to `maxSize` bytes and append a message indicating how many bytes were removed, e.g., `[truncated 42 bytes]`. If the string is within the limit, it should be returned unmodified.",
            "status": "done",
            "testStrategy": "Testing for this function will be defined in a separate subtask."
          },
          {
            "id": 3,
            "title": "Implement Unit Tests for `redactSecrets`",
            "description": "Create a suite of unit tests to validate the behavior of the `redactSecrets` function.",
            "dependencies": [
              "2.1"
            ],
            "details": "In a new test file, `src/utils/safety.test.ts`, write comprehensive unit tests for the `redactSecrets` function. Test cases should include: an object with sensitive keys (`apiKey`, `SECRET_TOKEN`), a deeply nested object with sensitive keys, an array of objects, an object with no sensitive keys (to ensure it remains unchanged), and non-object inputs to ensure graceful handling.",
            "status": "done",
            "testStrategy": "Use a testing framework like Jest or Vitest. Assert that the original object is not mutated and that the returned object has the correct values redacted."
          },
          {
            "id": 4,
            "title": "Implement Unit Tests for `capPayload`",
            "description": "Create a suite of unit tests to validate the behavior of the `capPayload` function.",
            "dependencies": [
              "2.2"
            ],
            "details": "In the `src/utils/safety.test.ts` file, add unit tests for the `capPayload` function. Cover the main scenarios: a string smaller than the 1MB threshold, a string larger than the threshold (verifying correct truncation and the appended message), and edge cases like an empty string or a string exactly at the threshold.",
            "status": "done",
            "testStrategy": "Verify both the returned string's content and its length to confirm the truncation logic is working as expected."
          },
          {
            "id": 5,
            "title": "Create and Integrate Secure Logger Wrapper",
            "description": "Create a logging utility that wraps the base logger to automatically redact secrets from log objects.",
            "dependencies": [
              "2.1"
            ],
            "details": "Based on the existing logging implementation, create a secure logger wrapper. This wrapper will expose standard logging methods (e.g., `info`, `warn`, `error`). Before passing a log object to the base logger, it must first process the object with the `redactSecrets` function created in subtask 2.1. This ensures that no sensitive data is ever written to the logs. This new utility should be exported for use throughout the application.",
            "status": "done",
            "testStrategy": "Manually inspect log output after integrating the new logger in a test script or a single application entry point to confirm that objects containing keys like 'token' are correctly redacted."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Resource Exposure",
        "description": "Load prompt metadata from `resources/prompts.meta.yaml` and expose each prompt's Markdown file as a `file://` resource.",
        "details": "On server startup, parse `prompts.meta.yaml`. For each entry, register a resource with the MCP server. The resource should have a human-friendly name (from the `title` field) and a `file://` URI pointing to the absolute path of the corresponding Markdown file. The resource content preview should be capped using the utility from task 2.",
        "testStrategy": "Start the server and use an MCP client to list resources. Verify that each prompt from the metadata file is listed with the correct name and a valid `file://` URI. Check that reading an oversized resource returns truncated content.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create a utility to parse `prompts.meta.yaml`",
            "description": "Add the `js-yaml` dependency to the project. Create a new utility function that reads the `resources/prompts.meta.yaml` file, parses its content, and returns a structured object. This function should handle potential file read or parsing errors gracefully.",
            "dependencies": [],
            "details": "Create a new file, e.g., `src/prompts/loader.ts`. Add a function `loadPromptMetadata()` that uses `fs.readFileSync` and `yaml.load`. Define a TypeScript interface for the expected structure of the YAML file (e.g., `{ prompts: [...] }`).",
            "status": "done",
            "testStrategy": "Add a unit test that uses a mock YAML file to ensure the parsing logic correctly converts YAML content to a JavaScript object."
          },
          {
            "id": 2,
            "title": "Implement logic to transform metadata into resource objects",
            "description": "Create a function that takes the parsed prompt metadata, iterates through each prompt entry, and transforms it into a `Resource` object as expected by the MCP server. This includes resolving the file path to an absolute `file://` URI.",
            "dependencies": [
              "3.1"
            ],
            "details": "In `src/prompts/loader.ts`, create a function like `preparePromptResources(metadata)`. For each prompt, use the `path` module to resolve the relative file path from `prompts.meta.yaml` to an absolute path. Format the absolute path as a `file://` URI. The resulting object should conform to the `Resource` interface, which includes `name` (from `title`) and `uri`.",
            "status": "done",
            "testStrategy": "Unit test this transformation logic to ensure file paths are correctly resolved to absolute `file://` URIs on different operating systems."
          },
          {
            "id": 3,
            "title": "Generate and cap content previews for each resource",
            "description": "Enhance the resource preparation logic to read the content of each prompt's Markdown file and generate a capped content preview using the utility from task 2.",
            "dependencies": [
              "3.2"
            ],
            "details": "Modify the function from the previous subtask. For each prompt, read the content of its Markdown file using `fs.readFileSync`. Import and use the `capContent` utility (assuming it's in `src/util/content.ts`) to truncate the file content. Add the resulting string to the `contentPreview` field of the `Resource` object.",
            "status": "done",
            "testStrategy": "Verify that a test resource with content larger than the cap is correctly truncated, and one with smaller content remains unchanged."
          },
          {
            "id": 4,
            "title": "Integrate resource registration into the server startup sequence",
            "description": "In the main server entry point, call the new functions to load, prepare, and register the prompt resources with the MCP server instance after it has been initialized.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Modify `src/main.ts`. After the `MCPServer` instance is created, call the prompt loading and preparation functions. Iterate over the generated list of `Resource` objects and call `mcpServer.registerResource()` for each one. This should happen before the server starts listening for connections.",
            "status": "done",
            "testStrategy": "Manually run the server and check the startup logs for any errors related to resource registration."
          },
          {
            "id": 5,
            "title": "Add an integration test to validate resource exposure",
            "description": "Create a new integration test that starts the server, uses an MCP client to request the list of all available resources, and validates that the prompts from `prompts.meta.yaml` are present with the correct details.",
            "dependencies": [
              "3.4"
            ],
            "details": "In a new test file, e.g., `test/integration/resource.test.ts`, write a test case that connects to the running server. It should call the `list_resources` tool. The test will then assert that the returned list contains entries corresponding to the prompts, verifying the `name`, `uri` (is a valid `file://` URI), and `contentPreview` (is a non-empty, capped string).",
            "status": "done",
            "testStrategy": "This subtask is the test itself. Ensure it covers at least two different prompts from the metadata file."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Dynamic Prompt Tools",
        "description": "Expose each prompt defined in `resources/prompts.meta.yaml` as a dynamically generated MCP tool.",
        "details": "During server startup, iterate through the entries in `prompts.meta.yaml`. For each entry, dynamically register an MCP tool with an `id` matching the metadata. Generate input/output schemas based on the metadata. The tool's handler should read the corresponding Markdown file from `resources/prompts/`, append a rendered footer, and return the content, applying the payload cap from task 2.",
        "testStrategy": "Use an MCP client to list tools and verify that a tool exists for each prompt in the metadata file. Invoke a tool and confirm the response contains the correct Markdown content. Test with a prompt file larger than 1 MB to ensure the response is truncated.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create a Utility to Load and Parse Prompt Metadata",
            "description": "Implement a function that reads `resources/prompts.meta.yaml`, parses it, and returns a validated, typed array of prompt metadata objects. This will serve as the single source of truth for prompt definitions.",
            "dependencies": [],
            "details": "Create a new file `src/lib/prompt-loader.ts`. Add an exported function `loadPromptDefinitions()`. This function should use the `fs` module to read `resources/prompts.meta.yaml` and the `js-yaml` library to parse its content. Define a TypeScript interface for the prompt metadata structure (e.g., `PromptDefinition`) and ensure the parsed data conforms to this type before returning it. This utility will be called during server startup.",
            "status": "done",
            "testStrategy": "Add a unit test to verify that the function correctly parses a sample YAML string and returns the expected array of objects."
          },
          {
            "id": 2,
            "title": "Develop a Generic Handler for Prompt Tools",
            "description": "Create a generic handler function that can be used by all dynamically generated prompt tools. The handler will be responsible for reading the prompt content, appending a footer, and applying the payload cap.",
            "dependencies": [],
            "details": "In a new file, e.g., `src/tools/prompt-handler.ts`, create a factory function `createPromptHandler(promptFilePath: string)`. This function should return an async `ToolHandler` function. The handler will read the file content from the provided `promptFilePath`, append a standard rendered footer (a simple string for now), and then apply the payload capping utility from Task 2 to the final content. The handler should return an object like `{ content: '...' }`.",
            "status": "done",
            "testStrategy": "Unit test the created handler to ensure it reads a file, appends the footer, and correctly truncates content that exceeds the payload cap."
          },
          {
            "id": 3,
            "title": "Implement Dynamic Schema Generation from Metadata",
            "description": "Create a function that generates JSON schemas for a tool's input and output based on the `variables` defined in its metadata.",
            "dependencies": [
              "4.1"
            ],
            "details": "In a new utility file, e.g., `src/lib/schema-generator.ts`, create a function `generateSchemas(metadata: PromptDefinition)`. This function will generate the `inputSchema` by creating a JSON Schema `object` with `properties` corresponding to each item in the metadata's `variables` array. The `outputSchema` should be a static JSON Schema defining an object with a single string property named `content`.",
            "status": "done",
            "testStrategy": "Unit test the schema generator with sample prompt metadata to ensure it produces valid input and output JSON schemas."
          },
          {
            "id": 4,
            "title": "Integrate Dynamic Tool Registration into Server Startup",
            "description": "Modify the server's startup sequence to iterate through the loaded prompt definitions and register an MCP tool for each one.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "In the primary tool registration file (e.g., `src/tools/tool-registry.ts`), create a new async function `registerPromptTools(mcpServer: McpServer)`. This function will call `loadPromptDefinitions()` (from subtask 4.1). It will then loop through each definition, calling `generateSchemas()` (subtask 4.3) and `createPromptHandler()` (subtask 4.2) for each. Finally, it will construct the complete `ToolDefinition` object (with `id`, schemas, and handler) and register it using `mcpServer.registerTool()`. Call this new function from the main server entry point (`src/server.ts`) during initialization.",
            "status": "done",
            "testStrategy": "After implementation, manually start the server and check the logs or use an MCP client to confirm that tools corresponding to `prompts.meta.yaml` are being registered without errors."
          },
          {
            "id": 5,
            "title": "Add Integration Tests for Dynamic Prompt Tools",
            "description": "Implement integration tests to verify that the dynamic tools are correctly exposed and functional through the MCP server.",
            "dependencies": [
              "4.4"
            ],
            "details": "In a new test file under `test/integration/`, write tests that use an MCP client to interact with the running server. One test should list all available tools and assert that a tool exists for each entry in `prompts.meta.yaml`. Another test should invoke a specific prompt tool and validate that the response body contains the expected markdown content. Add a final test using a large (>1MB) prompt file to ensure the response content is correctly truncated by the payload cap.",
            "status": "done",
            "testStrategy": "These are the validation tests themselves. Success is defined by all tests passing in the CI/CD pipeline."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Atomic State Store",
        "description": "Create a `StateStore` class to manage workflow state persistence in `.mcp/state.json` using atomic file writes.",
        "details": "Implement a class responsible for reading and writing the `ProjectState` JSON object. The `save` method must be atomic to prevent corruption. This should be achieved by writing the new state to a temporary file in the `.mcp/` directory and then using an atomic `rename` operation to replace the existing `state.json`. The store should also handle creating the `.mcp/` directory if it doesn't exist.",
        "testStrategy": "Write a test that simulates concurrent calls to the `save` method to ensure the final `state.json` file is always valid JSON and not a corrupted, partially written file. Verify that the directory is created if it's missing.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create StateStore Class and Directory Initialization Logic",
            "description": "Create the file `src/state/StateStore.ts` and define the `StateStore` class. The constructor should accept a project root path and ensure the `.mcp` directory exists.",
            "dependencies": [],
            "details": "Define the `StateStore` class in a new file `src/state/StateStore.ts`. The constructor will take `projectRoot: string`. It should define and store private properties for the paths to the `.mcp` directory, `state.json`, and a temporary file like `state.json.tmp`. Implement a private async method that is called by the constructor to create the `.mcp` directory using `fs.promises.mkdir(mcpDir, { recursive: true })`. This ensures all subsequent file operations have a valid directory to work in.",
            "status": "done",
            "testStrategy": "In a unit test, instantiate the class with a path to a temporary directory and assert that the `.mcp` subdirectory is created."
          },
          {
            "id": 2,
            "title": "Implement `load` Method to Read State from Disk",
            "description": "Implement an asynchronous `load` method to read and parse `.mcp/state.json`. It should handle cases where the file doesn't exist by providing a default initial state.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add a public async `load` method to the `StateStore` class. This method will attempt to read `.mcp/state.json` using `fs.promises.readFile`. If the file doesn't exist (catch the 'ENOENT' error), it should initialize a default `ProjectState` object: `{ completedTools: [], artifacts: {} }`. The loaded or default state should be stored in a private property (e.g., `_state`). The method should return the state.",
            "status": "done",
            "testStrategy": "Test that `load` returns the default state when `state.json` is missing. Create a mock `state.json` file and test that `load` correctly reads and parses its content."
          },
          {
            "id": 3,
            "title": "Implement In-Memory State Accessors and Mutators",
            "description": "Add methods to get the current state and to update it in memory, specifically by recording the completion of a tool. This prepares the store for use by other components like the `advance_state` tool.",
            "dependencies": [
              "5.1"
            ],
            "details": "Based on the `ProjectState` interface in `src/state/ProjectState.ts`, add a public getter `getState(): ProjectState` that returns a deep copy of the internal `_state` to prevent outside mutation. Also, add a public method `recordCompletion(completion: ToolCompletion)` which updates the internal `_state` by adding the new completion record to the `completedTools` array and merging the new artifacts into the top-level `artifacts` map.",
            "status": "done",
            "testStrategy": "Write a unit test that loads an initial state, calls `recordCompletion` with a mock `ToolCompletion` object, and then uses `getState` to verify that the in-memory state has been correctly updated."
          },
          {
            "id": 4,
            "title": "Implement Atomic `save` Method Using a Temporary File and Rename",
            "description": "Implement the `save` method to atomically persist the current in-memory state to `.mcp/state.json`.",
            "dependencies": [
              "5.3"
            ],
            "details": "Create a public async `save` method. This method will take the current in-memory state from the `_state` property, stringify it with `JSON.stringify`, and write it to the temporary file path (`.mcp/state.json.tmp`) using `fs.promises.writeFile`. Upon successful write, it will use `fs.promises.rename` to atomically move the temporary file to the final `state.json` path, overwriting any existing file. This two-step process prevents file corruption.",
            "status": "done",
            "testStrategy": "Verify that calling `save` creates `state.json` with the correct content. Check that the temporary file is created during the operation and removed after the rename is complete."
          },
          {
            "id": 5,
            "title": "Create Comprehensive Unit Tests for StateStore",
            "description": "Develop a suite of unit tests in `test/state/StateStore.test.ts` to validate all public methods and behaviors of the `StateStore` class.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4"
            ],
            "details": "Using a testing framework like Jest and a temporary file system utility, create a test file for the `StateStore`. The tests should cover: 1. Directory creation on instantiation. 2. Loading from a non-existent file. 3. Correctly saving state via `recordCompletion` and `save`. 4. Correctly loading a previously saved state. 5. Ensure the `getState` method returns a value-identical but not reference-identical object.",
            "status": "done",
            "testStrategy": "Execute the full test suite to ensure all features work as expected and are isolated from the actual project's file system. Mock the `fs` module to confirm the sequence of `writeFile` then `rename` is called for atomicity."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Planner `suggest_next_calls` Tool",
        "description": "Implement the `suggest_next_calls` tool to rank and suggest runnable tools based on DAG dependencies and available artifacts.",
        "details": "Create a `Planner` class that loads `resources/default-graph.json` and the current state from the `StateStore`. Implement the logic for the `suggest_next_calls` tool. This logic should compute which nodes in the DAG are 'ready' by checking if their `dependsOn` nodes are complete and their `requiresArtifacts` are present in the state. Return a ranked list of candidates, sorted by `phase` and then `id`.",
        "testStrategy": "Unit test the planner logic. Given an empty state, verify it returns `discover_research`. Simulate the completion of `discover_research` with a `research_summary` artifact and verify `define_prd` is now suggested. Test that a node with unmet artifact requirements is not returned.",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Planner Class and Load `default-graph.json`",
            "description": "Create a new file `src/planner.ts` and define a `Planner` class. The constructor should read `resources/default-graph.json`, parse it, and store the graph nodes in a private member. Also, define the necessary types for graph nodes in `src/types.ts`.",
            "dependencies": [],
            "details": "The class should have a private field `private graph: { nodes: any[] }`. The constructor will use `fs.readFileSync` and `JSON.parse` to load the graph data. The path to the graph file should be passed into the constructor. Add a `GraphNode` type definition to `src/types.ts` to represent the structure of nodes in the JSON file.",
            "status": "done",
            "testStrategy": "Unit test the constructor to ensure it correctly loads and parses the JSON file, and that the `graph` member is populated. Test for error handling if the file does not exist."
          },
          {
            "id": 2,
            "title": "Integrate `StateStore` into the `Planner` Class",
            "description": "Modify the `Planner` class to accept a `StateStore` instance in its constructor. Store the `StateStore` instance as a private member to allow access to the current project state for subsequent logic.",
            "dependencies": [
              "6.1"
            ],
            "details": "Update the `Planner` constructor signature to accept an instance of `StateStore` (from Task 5). Store the passed `stateStore` in a `private readonly stateStore: StateStore` field. This will be used by other methods to query the project's state.",
            "status": "done",
            "testStrategy": "Update the `Planner` unit tests to mock the `StateStore` and pass it to the constructor. Verify the instance is stored correctly within the `Planner` object."
          },
          {
            "id": 3,
            "title": "Implement `dependsOn` Completion Check in `Planner`",
            "description": "Create a private helper method within the `Planner` class, e.g., `areDependenciesMet(node: GraphNode): boolean`. This method should check if all task IDs listed in the node's `dependsOn` array are present in the list of completed tools from the `StateStore`.",
            "dependencies": [
              "6.2"
            ],
            "details": "The method will get the set of completed tool IDs from `this.stateStore.getCompletedToolIds()`. It will then iterate through the input node's `dependsOn` array and return `false` if any dependency ID is not in the completed set. If the `dependsOn` array is empty or all dependencies are met, it returns `true`.",
            "status": "done",
            "testStrategy": "Unit test this specific method. Provide a mock `StateStore` with a predefined set of completed tools and test nodes with met, unmet, and empty dependencies to ensure the logic is correct."
          },
          {
            "id": 4,
            "title": "Implement `requiresArtifacts` Availability Check in `Planner`",
            "description": "Create a private helper method, e.g., `areArtifactsAvailable(node: GraphNode): boolean`. This method should check if all artifacts listed in the node's `requiresArtifacts` array are available in the current state via the `StateStore`.",
            "dependencies": [
              "6.2"
            ],
            "details": "The method will get the set of available artifact names from `this.stateStore.getAvailableArtifacts()`. It will then iterate through the input node's `requiresArtifacts` array and return `false` if any required artifact is not in the available set. If `requiresArtifacts` is empty or all are available, it returns `true`.",
            "status": "done",
            "testStrategy": "Unit test this method. Provide a mock `StateStore` with a predefined set of available artifacts and test nodes with met, unmet, and empty artifact requirements."
          },
          {
            "id": 5,
            "title": "Implement `suggest_next_calls` to Filter and Rank Ready Nodes",
            "description": "Implement the public `suggest_next_calls` method. This method will iterate through all graph nodes, filter out already completed nodes, and use the `areDependenciesMet` and `areArtifactsAvailable` helpers to find 'ready' nodes. Finally, it will sort the ready nodes by `phase` (ascending) and then by `id` (alphabetical) before returning them.",
            "dependencies": [
              "6.3",
              "6.4"
            ],
            "details": "The method should first get the list of completed tool IDs from the state. Then, it will `filter` the graph nodes based on three conditions: 1) The node's ID is not in the completed list. 2) `this.areDependenciesMet(node)` is true. 3) `this.areArtifactsAvailable(node)` is true. The resulting array of nodes should then be sorted using a custom comparator for `phase` then `id`.",
            "status": "done",
            "testStrategy": "Write an integration test for `suggest_next_calls`. Given a graph and a mock `StateStore` in various states (e.g., empty, one task done, one task done with artifacts), verify that the correct list of ranked nodes is returned, respecting the sorting order."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement `advance_state` Tool",
        "description": "Create the `advance_state` MCP tool to mark a tool as complete and record its outputs and artifacts in the state file.",
        "details": "Register a new MCP tool named `advance_state`. Its handler will accept a tool `id`, `outputs`, and a list of `artifacts`. The handler will call a `recordCompletion` method on the `StateStore` instance, which updates the in-memory state by adding the completed tool and merging the new artifacts. The handler then triggers the `StateStore` to save the updated state to disk. It should return `{ ok: true }` and the path to the state file on success.",
        "testStrategy": "Call `advance_state` with a valid tool ID and artifact mapping. Inspect the resulting `.mcp/state.json` file to confirm it contains the new completion record with a timestamp, outputs, and the correct artifact paths.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `recordCompletion` Method in `StateStore`",
            "description": "Add a new public method `recordCompletion` to the `StateStore` class in `src/state/state-store.ts`. This method will accept a tool `id`, `outputs`, and a list of `artifacts`. It should create a `ToolCompletion` object with a new timestamp, add it to the `completedTools` map, and merge the new artifacts into the main `artifacts` map within the in-memory `ProjectState` object.",
            "dependencies": [],
            "details": "The method signature should be `recordCompletion(id: string, outputs: Record<string, any>, artifacts: Artifact[])`. It should not call `save()` itself; that responsibility will lie with the tool handler. The new artifacts should overwrite any existing artifacts with the same name. This method directly modifies the in-memory state managed by the `StateStore` instance.",
            "status": "done",
            "testStrategy": "Unit test the `recordCompletion` method. Initialize a `StateStore` with a blank state, call `recordCompletion` with sample data, and then inspect the internal `state` property to verify that `completedTools` and `artifacts` have been updated correctly and a timestamp has been added."
          },
          {
            "id": 2,
            "title": "Define `advance_state` Tool and Zod Input Schema",
            "description": "Create a new file `src/tools/definitions/advance-state.ts`. In this file, define and export the Zod schema for the `advance_state` tool's input, which includes a string `id`, a `z.record(z.any())` for `outputs`, and an array of `artifact` objects for `artifacts`. Also, define the `McpTool` object with the name `advance_state`, a suitable description, and a reference to the schema.",
            "dependencies": [],
            "details": "The artifact schema within the input array should validate `source`, `name`, and `uri` as strings, consistent with the `Artifact` type in `src/state/state-types.ts`. The main tool object will be exported to be used for registration later. The handler function can be a placeholder for now.",
            "status": "done",
            "testStrategy": "No dedicated test is needed for the definition itself, as it will be validated by the tool registry and tested implicitly by the handler's tests."
          },
          {
            "id": 3,
            "title": "Implement `advance_state` Handler to Call `recordCompletion`",
            "description": "In `src/tools/definitions/advance-state.ts`, implement the body of the `handler` function for the `advance_state` tool. The handler will use the `stateStore` from its context to call the `recordCompletion` method, passing the validated `id`, `outputs`, and `artifacts` from its input.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "The handler will receive `context: McpToolContext` and `input: { id: string, ... }`. It should destructure these arguments and call `context.stateStore.recordCompletion(input.id, input.outputs, input.artifacts)`. The handler function should be marked as `async`.",
            "status": "done",
            "testStrategy": "Using a mock `StateStore` in a unit test, verify that the handler correctly calls the `recordCompletion` method with the exact arguments passed into the tool."
          },
          {
            "id": 4,
            "title": "Add State Persistence and Success Response to `advance_state` Handler",
            "description": "Extend the `advance_state` handler in `src/tools/definitions/advance-state.ts`. After calling `recordCompletion`, the handler must call `await context.stateStore.save()` to persist the updated state to disk. On success, it should return an object `{ ok: true, statePath: <path_to_state.json> }`.",
            "dependencies": [
              "7.3"
            ],
            "details": "The `save()` method on `StateStore` is asynchronous and must be awaited. The path to the state file can be retrieved from a property on the `stateStore` instance (e.g., `context.stateStore.statePath`). This completes the core logic of the tool.",
            "status": "done",
            "testStrategy": "In an integration test, call the tool handler and verify that the `save` method on the `StateStore` mock is called. Also, check that the handler's return value matches the specified success format."
          },
          {
            "id": 5,
            "title": "Register `advance_state` Tool with the MCP Server",
            "description": "Import the `advance_state` tool definition from `src/tools/definitions/advance-state.ts` into the main tool registration location (likely `src/mcp/mcp-server.ts` or a dedicated tool loading module). Register the tool with the `ToolRegistry` instance so it becomes available to the MCP server.",
            "dependencies": [
              "7.2"
            ],
            "details": "Locate the code block where other tools are registered, which is typically in the `McpServer` constructor or an initialization method. Add a line like `this.toolRegistry.registerTool(advanceStateTool);` to include the new tool in the server's list of available tools.",
            "status": "done",
            "testStrategy": "Start the server and use an MCP client or a test utility to list available tools. Verify that `advance_state` is present in the list."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement `export_task_list` Tool",
        "description": "Create the `export_task_list` tool to emit a compact task list for interoperability with external systems.",
        "details": "Register a new MCP tool named `export_task_list`. The handler will read `resources/prompts.meta.yaml` and map its contents to a JSON array. Each element in the array should contain `id`, `title`, `dependsOn`, and a default `status: 'pending'`. This provides a simple, structured view of the entire workflow.",
        "testStrategy": "Call the `export_task_list` tool. Validate that the returned JSON array contains an entry for every prompt defined in the metadata file and that the `id`, `title`, and `dependsOn` fields correctly match the source data.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `export_task_list` Tool Skeleton",
            "description": "Create the file `src/tools/export_task_list.ts` and define the basic structure for the new tool, including the factory function `createExportTaskListTool` and a placeholder implementation.",
            "dependencies": [],
            "details": "Following the existing pattern in `src/tools`, create a new file for the `export_task_list` tool. It should export a factory function that returns a `Tool` object with the name 'export_task_list', a clear description, an empty input schema, and a handler that currently returns an empty array. This establishes the boilerplate for the tool.",
            "status": "done",
            "testStrategy": "Verify the new file `src/tools/export_task_list.ts` exists and exports the `createExportTaskListTool` function. Ensure the returned tool object has the correct name."
          },
          {
            "id": 2,
            "title": "Implement YAML File Reading and Parsing",
            "description": "Update the `export_task_list` tool handler to read and parse the `resources/prompts.meta.yaml` file.",
            "dependencies": [
              "8.1"
            ],
            "details": "Use the `fs` module to read the contents of `resources/prompts.meta.yaml`. Add the `js-yaml` library as a dependency if it's not already part of the project. Use the library to parse the YAML content into a JavaScript array of objects. Implement basic error handling for file-not-found scenarios.",
            "status": "done",
            "testStrategy": "In a test environment, mock `fs.readFileSync` to return a sample YAML string. Call the handler and assert that it correctly parses the string into a JavaScript object without errors."
          },
          {
            "id": 3,
            "title": "Map Parsed YAML to the Specified JSON Array Format",
            "description": "Transform the parsed data from `prompts.meta.yaml` into the final JSON array structure required by the task.",
            "dependencies": [
              "8.2"
            ],
            "details": "Within the tool's handler, iterate over the array of parsed prompt objects. For each object, create a new object containing the `id`, `title`, and `dependsOn` fields from the source. Add a `status` field with a default value of 'pending'. The handler should return this newly created array.",
            "status": "done",
            "testStrategy": "Write a unit test for the handler that provides a pre-parsed array of prompt objects. Verify that the returned array has the correct length and that each element contains the `id`, `title`, `dependsOn`, and `status` fields with the expected values."
          },
          {
            "id": 4,
            "title": "Register the Tool with the MCP Server",
            "description": "Integrate the newly created `export_task_list` tool into the main application by registering it with the MCP server on startup.",
            "dependencies": [
              "8.1"
            ],
            "details": "In `src/main.ts`, import the `createExportTaskListTool` factory function. In the main server initialization block, call the factory and pass the resulting tool object to the `server.registerTool()` method. This will make the tool available via the MCP.",
            "status": "done",
            "testStrategy": "After starting the server, use an MCP client or a test utility to list all available tools. Confirm that `export_task_list` appears in the list of registered tools."
          },
          {
            "id": 5,
            "title": "Create an Integration Test for the `export_task_list` Tool",
            "description": "Write a test that invokes the complete `export_task_list` tool and validates its output against a known `prompts.meta.yaml` file.",
            "dependencies": [
              "8.3",
              "8.4"
            ],
            "details": "Create a test file `src/tools/export_task_list.test.ts`. The test should execute the tool's handler. It should use a test-specific version of `prompts.meta.yaml` to ensure a stable test environment. The test will assert that the returned JSON array correctly reflects the contents of the test YAML file, verifying the entire flow from file reading to data transformation.",
            "status": "done",
            "testStrategy": "Run the test suite. The test should pass if the handler correctly reads a mock YAML file, transforms its content, and returns a JSON array matching the expected structure and values."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Token Bucket Rate Limiter Utility",
        "description": "Provide a `TokenBucket` utility for rate limiting, intended for future use with external HTTP integrations.",
        "status": "deprecated",
        "dependencies": [
          1
        ],
        "priority": "low",
        "details": "Create a `TokenBucket` class with a constructor that accepts `capacity` and `refillPerSec`. Implement a `take(count)` method that decrements the token count. If not enough tokens are available, the method should asynchronously wait for the required number of tokens to be refilled before resolving. This is a foundational component for future safety controls.",
        "testStrategy": "Unit test the `TokenBucket`. Verify that calling `take(1)` when the bucket is empty causes a delay approximately equal to `1 / refillPerSec`. Test that taking the full capacity immediately depletes the tokens and that they recover over time as expected.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `TokenBucket.ts` with Class Skeleton and Constructor",
            "description": "Create the file `src/utils/TokenBucket.ts`. Define the `TokenBucket` class with a constructor that accepts `capacity` and `refillPerSec`. Initialize class properties for `capacity`, `refillPerSec`, `tokens`, and `lastRefillTime`.",
            "dependencies": [],
            "details": "This subtask establishes the foundational file and class structure. The constructor should initialize `this.tokens` to `this.capacity` and `this.lastRefillTime` to the current time (`Date.now()`). Also, create the corresponding test file `tests/TokenBucket.test.ts` with a placeholder test suite.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Private `_refill` Method for Token Replenishment",
            "description": "Implement a private helper method, `_refill()`, inside the `TokenBucket` class. This method will calculate the number of new tokens to add based on the time elapsed since `lastRefillTime` and the `refillPerSec` rate.",
            "dependencies": [
              "9.1"
            ],
            "details": "The `_refill` method should calculate elapsed time since the last refill, determine the number of tokens to add, and update `this.lastRefillTime` to the current time. It must ensure that the number of tokens does not exceed the `capacity`. This isolates the core state update logic.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Synchronous Path for `take(count)` Method",
            "description": "Implement the `take(count)` method. It should first call the `_refill()` method to ensure the token count is up-to-date. Then, it should handle the synchronous case where sufficient tokens are available.",
            "dependencies": [
              "9.2"
            ],
            "details": "The `take` method will be `async`. After calling `_refill()`, if `this.tokens >= count`, the method should decrement `this.tokens` by `count` and resolve immediately. This covers the 'happy path' where no waiting is necessary.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Asynchronous Waiting Logic in `take(count)`",
            "description": "Enhance the `take(count)` method to handle cases where there are insufficient tokens. The method should calculate the necessary delay until enough tokens are available and wait asynchronously before proceeding.",
            "dependencies": [
              "9.3"
            ],
            "details": "When `this.tokens < count`, calculate the `tokensNeeded` and the `waitTime` in milliseconds. Use `new Promise(resolve => setTimeout(resolve, waitTime))` to pause execution. After the delay, recursively call `this.take(count)` to re-evaluate and consume the tokens.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Comprehensive Unit Tests in `tests/TokenBucket.test.ts`",
            "description": "Implement unit tests in `tests/TokenBucket.test.ts` to validate the `TokenBucket` functionality, including asynchronous behavior.",
            "dependencies": [
              "9.4"
            ],
            "details": "Tests should cover: 1. Initial state (full bucket). 2. Immediate depletion of tokens. 3. Correct asynchronous delay when taking from an empty bucket. 4. Gradual refill over time. Use a testing framework like Jest with fake timers to control time during tests and make assertions on wait times.",
            "status": "pending",
            "testStrategy": "Using a test runner like Jest, mock timers to verify that `take(1)` on an empty bucket with `refillPerSec: 10` waits approximately 100ms. Test that taking full capacity works, and subsequent takes wait for the correct refill duration."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 10,
        "title": "Implement Prompt Metadata Validation Script",
        "description": "Flesh out the `scripts/validate_metadata.ts` script to parse YAML front matter from all markdown prompt files and validate it against a strict schema.",
        "details": "The script must be executed via `npm run validate:metadata`. It should read all `.md` files in the prompts directory, extract the YAML front matter, and ensure required fields (e.g., `id`, `title`, `phase`, `description`) are present and correctly typed. The script should exit with a non-zero code if validation fails, logging clear error messages about which file and field are invalid. This aligns with the 'Prompt Metadata Automation' feature and is a prerequisite for building the catalog.",
        "testStrategy": "Create a temporary valid prompt file and an invalid one (e.g., missing a required field). Run the script against them. Verify the script passes for the valid file and fails with a descriptive error for the invalid one. Ensure it correctly handles various data types.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Dependencies and Configure `validate:metadata` NPM Script",
            "description": "Add `zod`, `gray-matter`, and `glob` as development dependencies to `package.json`. Ensure the `validate:metadata` script in `package.json` is correctly configured to execute `scripts/validate_metadata.ts` using `ts-node`.",
            "dependencies": [],
            "details": "This foundational step ensures all required tools are available before writing the core logic. The command should be `npm install zod gray-matter glob --save-dev`. The script should be `\"validate:metadata\": \"ts-node scripts/validate_metadata.ts\"`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Define Prompt Metadata Schema using Zod",
            "description": "In `scripts/validate_metadata.ts`, define a strict Zod schema for the prompt's YAML front matter. The schema must enforce the presence and correct types for `id` (string), `title` (string), `phase` (number), and `description` (string).",
            "dependencies": [
              "10.1"
            ],
            "details": "Create a `const promptMetadataSchema = z.object({...})`. This schema will be the single source of truth for metadata validation and should be placed at the top of the script file for clarity.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement File Discovery for Prompt Markdown Files",
            "description": "In `scripts/validate_metadata.ts`, use the `glob` library to implement a function that finds and returns a list of all `.md` file paths within the `prompts/` directory and its subdirectories.",
            "dependencies": [
              "10.1"
            ],
            "details": "The function should asynchronously find all files matching the pattern `prompts/**/*.md`. This isolates the file system interaction from the parsing and validation logic. The main script function will await the result of this discovery.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Main Loop to Parse and Validate Front Matter",
            "description": "Create the main execution logic in `scripts/validate_metadata.ts`. This logic should iterate through the file paths discovered in subtask 3, read each file's content, use `gray-matter` to parse the YAML front matter, and then validate the resulting object against the Zod schema defined in subtask 2.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "This subtask ties together file discovery, parsing, and validation. It will form the core of the script's functionality. Use `fs.readFileSync` to get file content and `matter()` from `gray-matter` to extract the `data` object for validation.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Error Reporting and Non-Zero Exit Code on Failure",
            "description": "Enhance the validation loop to handle validation failures. When a file's metadata is invalid, log a clear error message to the console specifying the file path and the nature of the validation error. The script must exit with `process.exit(1)` if any validation errors are found.",
            "dependencies": [
              "10.4"
            ],
            "details": "Use a `try...catch` block around the Zod `safeParse` method. If the parse result is not successful, format the `ZodError` into a user-friendly message. Track a global error flag or a count of errors to determine the final exit code after checking all files.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Catalog and README Build Script",
        "description": "Develop the `scripts/build_catalog.ts` script to automatically generate `catalog.json` and update the prompt tables in `README.md` based on the metadata from all prompt files.",
        "details": "This script, run via `npm run build:catalog`, will consume the validated metadata from all `.md` prompt files. It will then generate a `catalog.json` file containing an array of all prompt metadata objects. Concurrently, it will parse `README.md`, find placeholder tags, and inject updated markdown tables, ensuring the README stays synchronized with the prompt library. This fulfills a core requirement of 'Prompt Metadata Automation'.",
        "testStrategy": "After creating a few sample prompts, run the script. Check that `catalog.json` is created or updated correctly. Inspect `README.md` to confirm the prompt tables are accurately regenerated. Verify that running the script a second time without changes results in no file modifications.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create a Utility to Discover All Prompt Markdown Files",
            "description": "Within the new `scripts/build_catalog.ts` file, implement a utility function that finds all prompt markdown files. This function will use a glob pattern to recursively search the `prompts/` directory for all files ending in `.md` and return an array of their full paths.",
            "dependencies": [],
            "details": "This is the initial step for gathering the source files. A library like `glob` or `fast-glob` should be added as a dev dependency to handle the file system traversal. The function should be designed to be reusable within the script.",
            "status": "done",
            "testStrategy": "Create a temporary directory with a few nested `.md` files and some other file types. Assert that the utility function returns the correct paths for only the `.md` files."
          },
          {
            "id": 2,
            "title": "Implement a Parser to Extract and Validate Prompt Front-Matter",
            "description": "Create a function that accepts a file path, reads the markdown file's content, and parses its YAML front-matter using the `gray-matter` library. The extracted metadata object should then be validated against a predefined schema to ensure it contains all required fields (e.g., id, title, description).",
            "dependencies": [
              "11.1"
            ],
            "details": "This function is critical for consuming the prompt files discovered in the previous subtask. A validation library like `zod` should be used to define and enforce the metadata structure. The function should throw a clear error or log a warning if a file is missing front-matter or if the metadata is invalid.",
            "status": "done",
            "testStrategy": "Test with a valid prompt file and assert the metadata is parsed correctly. Test with a file missing front-matter and another with invalid/missing fields to ensure errors are handled gracefully."
          },
          {
            "id": 3,
            "title": "Develop Main Script Logic to Aggregate Metadata from All Prompts",
            "description": "Implement the main execution logic in `scripts/build_catalog.ts`. This logic will use the discovery utility (11.1) to get all prompt file paths, then iterate through them, calling the parser (11.2) on each. All successfully parsed and validated metadata objects should be collected into a single in-memory array.",
            "dependencies": [
              "11.1",
              "11.2"
            ],
            "details": "This subtask orchestrates the data collection process. The main function should handle errors from the parsing step by logging the problematic file and continuing, ensuring that one bad file doesn't halt the entire process. The final result is a complete, in-memory representation of the prompt catalog.",
            "status": "done",
            "testStrategy": "Using a set of mock prompt files (including one invalid one), run the aggregation logic and assert that the final array contains the correct metadata for the valid files and that a warning was logged for the invalid one."
          },
          {
            "id": 4,
            "title": "Implement `catalog.json` Generation from Aggregated Metadata",
            "description": "Create a function that takes the aggregated array of prompt metadata (from 11.3), sorts it by prompt ID, serializes it into a human-readable JSON string, and writes it to the `catalog.json` file in the project root.",
            "dependencies": [
              "11.3"
            ],
            "details": "This function is responsible for the first major output of the script. It should use `JSON.stringify` with an indentation setting for pretty-printing. The file write operation should overwrite the existing `catalog.json` to ensure it's always up-to-date.",
            "status": "done",
            "testStrategy": "Provide a sample metadata array to the function. After it runs, read the generated `catalog.json` and assert that its content matches the expected formatted JSON output and is correctly sorted."
          },
          {
            "id": 5,
            "title": "Implement README.md Update with Auto-Generated Prompt Table",
            "description": "Develop a function that generates a Markdown table from the aggregated prompt metadata. This function must then read `README.md`, find the `<!-- PROMPT_TABLE_START -->` and `<!-- PROMPT_TABLE_END -->` comment tags, and replace all content between them with the newly generated table.",
            "dependencies": [
              "11.3"
            ],
            "details": "This function ensures the project's documentation stays synchronized with the prompt library. The table should include columns for ID, Title, and Description. The file update logic must be precise, likely using a regular expression, to avoid modifying any other part of the `README.md` file.",
            "status": "done",
            "testStrategy": "Create a mock `README.md` file with the placeholder comments and surrounding text. Run the function with sample metadata and assert that only the content between the placeholders is replaced with the correct markdown table."
          }
        ]
      },
      {
        "id": 12,
        "title": "Create DocFetch Preflight Guardrail Prompt",
        "description": "Author the primary slash-command prompt that enforces the DocFetch preflight check before planning or coding can proceed.",
        "details": "Create a markdown prompt file (e.g., `prompts/preflight/docfetch-check.md`) with appropriate YAML front matter. The prompt's content should instruct the user on how to run the DocFetch tooling, validate the status of the `DocFetchReport`, and provide clear, actionable remediation steps if documentation sources are missing or stale. This directly implements the 'DocFetch Preflight Guardrails' feature.",
        "testStrategy": "Execute the prompt via the Codex CLI. Verify that its output clearly states the success criteria (an OK DocFetchReport) and provides a clear remediation path for failure cases. Ensure the prompt's metadata is correctly processed by the validation and build scripts.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `docfetch-check.md` and Define YAML Front Matter",
            "description": "Create the markdown file for the DocFetch preflight prompt and add the necessary YAML front matter to define its metadata, following the structure suggested in the task description.",
            "dependencies": [],
            "details": "Create the file at `prompts/preflight/docfetch-check.md`. Based on the parent task and similar prompts, the YAML front matter should include `id`, `title`, `description`, and `phase: preflight`. This establishes the prompt's identity within the system and associates it with the preflight check phase of the workflow.",
            "status": "done",
            "testStrategy": "Verify the file `prompts/preflight/docfetch-check.md` is created. Manually inspect the YAML front matter to ensure it contains the required keys (`id`, `title`, `description`, `phase`) with appropriate values."
          },
          {
            "id": 2,
            "title": "Draft Introduction Explaining the DocFetch Check's Purpose",
            "description": "Write the introductory section of the prompt that explains to the user why the DocFetch preflight check is a necessary guardrail before proceeding with planning or coding.",
            "dependencies": [
              "12.1"
            ],
            "details": "In `prompts/preflight/docfetch-check.md`, add a section below the front matter. This content should clearly explain that planning and coding require up-to-date project context from documentation, and that the DocFetch tool is designed to validate this context automatically.",
            "status": "done",
            "testStrategy": "Read the rendered markdown. The introduction should be clear, concise, and effectively communicate the importance of having fresh documentation context for the AI to perform its tasks."
          },
          {
            "id": 3,
            "title": "Add Instructions for Executing the DocFetch Tool",
            "description": "Add a section to the prompt detailing the specific command(s) required to run the DocFetch tool and generate the `DocFetchReport`.",
            "dependencies": [
              "12.2"
            ],
            "details": "In `prompts/preflight/docfetch-check.md`, create a clear, user-friendly section titled 'How to Run the Check'. Provide the exact command-line instruction to trigger the documentation fetching and analysis process. Use markdown code blocks for the command to make it easy to copy and paste.",
            "status": "done",
            "testStrategy": "Review the prompt to ensure the command for running the DocFetch tool is present, correct, and clearly formatted within a code block."
          },
          {
            "id": 4,
            "title": "Document How to Interpret the `DocFetchReport`",
            "description": "Add content that guides the user on how to find and interpret the `DocFetchReport` to determine if the check passed or failed.",
            "dependencies": [
              "12.3"
            ],
            "details": "In `prompts/preflight/docfetch-check.md`, add a section explaining where the `DocFetchReport` is located (e.g., `.codex/reports/docfetch-report.json`). Describe what a successful report looks like (e.g., `status: 'OK'`) versus what a failure looks like (e.g., `status: 'STALE'`, `missingSources: [...]`). This section is crucial for user self-service.",
            "status": "done",
            "testStrategy": "Verify the prompt clearly explains the location of the report and provides distinct examples of 'success' and 'failure' states based on the report's content."
          },
          {
            "id": 5,
            "title": "Author Remediation Steps for Failed DocFetch Checks",
            "description": "Write the final section of the prompt, providing clear, actionable steps to resolve a failed DocFetch check based on the report's findings.",
            "dependencies": [
              "12.4"
            ],
            "details": "In `prompts/preflight/docfetch-check.md`, create a 'Remediation Steps' or 'What to Do If It Fails' section. This should provide specific instructions for different failure scenarios, such as how to add missing documentation paths to the project's configuration file or how to force-refresh stale sources before re-running the DocFetch tool.",
            "status": "done",
            "testStrategy": "Check that the remediation steps are actionable and cover the likely failure modes (e.g., missing sources, stale files). The instructions should be clear enough for a user to follow without assistance."
          }
        ]
      },
      {
        "id": 13,
        "title": "Author Planning Phase Prompts",
        "description": "Create at least one slash-command prompt for the 'planning' phase of the development lifecycle.",
        "details": "As part of the 'Lifecycle Prompt Library', author a markdown prompt with YAML front matter indicating `phase: planning`. The prompt should guide a user through a planning activity, such as breaking down a feature or defining acceptance criteria, consistent with the `WORKFLOW.md` document.",
        "testStrategy": "Run the prompt and check that its output is directive, concise, and aligns with the planning stage. Verify its metadata is correctly added to `catalog.json` and the `README.md` tables after running the build scripts.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Research Planning Phase Requirements in WORKFLOW.md",
            "description": "Analyze the 'Planning' section of WORKFLOW.md to understand the key activities, such as feature breakdown and defining acceptance criteria. This research will define the scope and purpose of the new planning prompt.",
            "dependencies": [],
            "details": "Read the WORKFLOW.md file to identify the specific goals and outputs expected during the planning phase of the development lifecycle. This will ensure the prompt aligns with the established process.",
            "status": "pending",
            "testStrategy": "Confirm that the documented planning activities in WORKFLOW.md are understood and can be translated into a prompt."
          },
          {
            "id": 2,
            "title": "Create Initial Draft of `feature-breakdown.md` Prompt",
            "description": "Create a new markdown file at `src/prompts/commands/planning/feature-breakdown.md`. Draft the initial version of the prompt's instructional text to guide a user through breaking down a feature into actionable tasks.",
            "dependencies": [
              "13.1"
            ],
            "details": "Based on the research from the previous subtask, write the main body of the prompt. The text should instruct the AI on how to analyze a feature request and output a structured plan.",
            "status": "pending",
            "testStrategy": "Review the draft to ensure it contains clear instructions for an AI to follow."
          },
          {
            "id": 3,
            "title": "Add YAML Front Matter to `feature-breakdown.md`",
            "description": "Add the required YAML front matter to the `src/prompts/commands/planning/feature-breakdown.md` file. This includes a `title`, `description`, and `phase: planning`.",
            "dependencies": [
              "13.2"
            ],
            "details": "The front matter is critical for the prompt to be recognized by the system. The title should be actionable, like 'Feature Breakdown', and the description should clearly state its purpose. The `phase` must be set to `planning`.",
            "status": "pending",
            "testStrategy": "Inspect the raw markdown file to ensure the YAML front matter is present, correctly formatted, and contains all required fields."
          },
          {
            "id": 4,
            "title": "Refine and Test the Feature Breakdown Prompt",
            "description": "Iteratively test the `feature-breakdown.md` prompt by running it with a sample feature description. Refine the prompt's text to ensure the output is structured, clear, and consistently provides actionable subtasks and acceptance criteria.",
            "dependencies": [
              "13.3"
            ],
            "details": "This step involves practical testing of the prompt's effectiveness. The goal is to fine-tune the instructions until the AI's output reliably meets the standards defined in WORKFLOW.md for the planning phase.",
            "status": "pending",
            "testStrategy": "Execute the prompt with a sample input and verify the output is a well-structured plan with clear subtasks and acceptance criteria."
          },
          {
            "id": 5,
            "title": "Run Build Scripts and Verify Prompt Integration",
            "description": "Execute the project's build scripts to update the prompt catalog and documentation. Verify that the new 'Feature Breakdown' prompt is correctly added to `catalog.json` and the relevant tables in `README.md`.",
            "dependencies": [
              "13.4"
            ],
            "details": "The final step is to ensure the new prompt is properly integrated into the project's ecosystem. This involves running any scripts responsible for cataloging and documenting available prompts.",
            "status": "pending",
            "testStrategy": "After running the build scripts, check the contents of `catalog.json` and `README.md` to confirm the new prompt is listed with the correct metadata."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 14,
        "title": "Author Scaffolding Phase Prompts",
        "description": "Create at least one slash-command prompt for the 'scaffolding' phase of the development lifecycle.",
        "details": "As part of the 'Lifecycle Prompt Library', author a markdown prompt with YAML front matter indicating `phase: scaffolding`. The prompt should assist a developer in generating boilerplate code, setting up file structures, or creating initial components based on a plan.",
        "testStrategy": "Run the prompt and check that its output helps accelerate initial code creation. Verify its metadata is correctly added to `catalog.json` and the `README.md` tables after running the build scripts.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Scope for '/scaffold-component' Prompt",
            "description": "Define the specific scope and target for the new scaffolding prompt. Decide on the component type, language, and framework to be scaffolded, as recommended by the complexity report. This initial definition will guide the subsequent implementation.",
            "dependencies": [],
            "details": "Based on the complexity report's recommendation, the target will be a React functional component using TypeScript. Document the expected inputs (e.g., component name, list of props) and the desired output structure (e.g., a single `.tsx` file with boilerplate code, prop types, and basic JSX). This definition serves as the design document for the prompt.",
            "status": "pending",
            "testStrategy": "Review the definition to ensure it is specific enough to create a concrete and useful scaffolding prompt."
          },
          {
            "id": 2,
            "title": "Create 'scaffold-component.md' with YAML Frontmatter",
            "description": "Create the new prompt file at 'prompts/library/scaffolding/scaffold-component.md' and add the YAML frontmatter based on the scope defined in the previous subtask.",
            "dependencies": [
              "14.1"
            ],
            "details": "Create the file `prompts/library/scaffolding/scaffold-component.md`. The YAML frontmatter should include `title: Scaffold Component`, `description`, `author`, `tags: [\"scaffolding\", \"react\", \"typescript\", \"component\"]`, and `phase: scaffolding`. The description should clearly state that it generates a React functional component with TypeScript.",
            "status": "pending",
            "testStrategy": "Verify the file is created in the correct directory and the YAML frontmatter is valid and contains all required fields."
          },
          {
            "id": 3,
            "title": "Write the Main Prompt Body for '/scaffold-component'",
            "description": "Write the main body of the prompt in 'scaffold-component.md'. This section should contain the core instructions for the AI on how to generate the component.",
            "dependencies": [
              "14.2"
            ],
            "details": "In the markdown file, add 'System' and 'Instructions' sections. The instructions should guide the AI to: 1. Accept a component name and a list of props as input. 2. Generate a TypeScript interface for the props. 3. Create a React functional component using the provided name and props interface. 4. Include basic JSX structure. 5. Return the output as a single, formatted code block.",
            "status": "pending",
            "testStrategy": "Read through the instructions to ensure they are clear, unambiguous, and logically lead to the desired code output."
          },
          {
            "id": 4,
            "title": "Add a Detailed Example to 'scaffold-component.md'",
            "description": "Add a comprehensive 'Example' section to the 'scaffold-component.md' prompt file to demonstrate its usage and expected output.",
            "dependencies": [
              "14.3"
            ],
            "details": "Append an '## Example' section to the markdown file. The example should show a sample user request (e.g., `/scaffold-component name:UserProfile props:name:string,age:number`) and the corresponding complete AI response, including the generated `.tsx` code for the `UserProfile` component.",
            "status": "pending",
            "testStrategy": "Verify the example clearly illustrates the prompt's functionality and the format of the expected output code."
          },
          {
            "id": 5,
            "title": "Test and Verify '/scaffold-component' Prompt Integration",
            "description": "Test the completed prompt by running it in a development environment. Verify the generated code is correct and that the prompt is correctly integrated into the project's documentation.",
            "dependencies": [
              "14.4"
            ],
            "details": "Execute the `/scaffold-component` prompt with the example inputs. Check if the output is a valid React/TypeScript component. After confirming functionality, run the project's build scripts. Verify that the new prompt is added to the `catalog.json` file and appears in the appropriate table in the `README.md`.",
            "status": "pending",
            "testStrategy": "The generated code from the prompt should be syntactically correct. The `catalog.json` and `README.md` files must reflect the addition of the new prompt after the build scripts are run."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 15,
        "title": "Author Testing Phase Prompts",
        "description": "Create at least one slash-command prompt for the 'testing' phase of the development lifecycle.",
        "details": "As part of the 'Lifecycle Prompt Library', author a markdown prompt with YAML front matter indicating `phase: testing`. The prompt should guide a developer in writing unit tests, integration tests, or generating test case tables.",
        "testStrategy": "Run the prompt and confirm its output provides useful guidance for creating tests. Verify its metadata is correctly added to `catalog.json` and the `README.md` tables after running the build scripts.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Create initial prompt file for test case generation",
            "description": "Following the pattern observed in `prompts/lifecycle/`, create the initial markdown file for the test case generation prompt.",
            "dependencies": [],
            "details": "Create a new file at `prompts/lifecycle/testing-generate-cases.md`. Add the standard YAML front matter including `name: /test/generate-cases`, a suitable `description`, and `phase: testing`. The main body of the prompt can be a placeholder for now.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop prompt content for generating test case tables in 'testing-generate-cases.md'",
            "description": "Flesh out the markdown content for the `testing-generate-cases.md` prompt to guide an AI in creating structured test case tables.",
            "dependencies": [
              "15.1"
            ],
            "details": "In the file `prompts/lifecycle/testing-generate-cases.md`, write a detailed prompt that instructs the AI to analyze user stories, requirements, or code snippets and produce a markdown table of test cases. The table should include columns for Test ID, Description, Steps, Expected Result, and Type (e.g., Happy Path, Edge Case, Error).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create and author the 'write-unit-tests' prompt",
            "description": "Create a new, self-contained prompt file dedicated to generating unit tests for a specific function or module.",
            "dependencies": [],
            "details": "Create the file `prompts/lifecycle/testing-unit-tests.md`. Add YAML front matter with `name: /test/unit-tests`, a description, and `phase: testing`. The prompt content should guide the AI to write unit tests for a provided code snippet, including instructions on mocking dependencies and asserting expected outcomes using a common testing framework like Jest or Pytest.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create and author the 'write-integration-tests' prompt",
            "description": "Create a new, self-contained prompt file for generating integration tests between multiple components.",
            "dependencies": [],
            "details": "Create the file `prompts/lifecycle/testing-integration-tests.md`. Add YAML front matter with `name: /test/integration-tests`, a description, and `phase: testing`. The prompt content should instruct the AI to write tests that verify the interaction, data flow, and contract between two or more specified components or services.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Run build scripts and verify all new testing prompts in catalog",
            "description": "After the three new testing prompts are authored, run the build process to update the prompt catalog and documentation, and verify their inclusion.",
            "dependencies": [
              "15.2",
              "15.3",
              "15.4"
            ],
            "details": "Execute the `npm run build:prompts` script (or equivalent build script). Check the generated `prompts/catalog.json` file to ensure the three new prompts (`/test/generate-cases`, `/test/unit-tests`, `/test/integration-tests`) are present with correct metadata. Also, verify they are listed in the appropriate table in the main `README.md`.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 16,
        "title": "Author Release Phase Prompts",
        "description": "Create at least one slash-command prompt for the 'release' phase of the development lifecycle.",
        "details": "As part of the 'Lifecycle Prompt Library', author a markdown prompt with YAML front matter indicating `phase: release`. The prompt should assist with release-gating activities like generating release notes, creating a changelog, or verifying final checks.",
        "testStrategy": "Run the prompt and check that its output supports pre-release activities. Verify its metadata is correctly added to `catalog.json` and the `README.md` tables after running the build scripts.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Create /generate-release-notes prompt file with initial YAML front matter",
            "description": "Based on the analysis of existing prompts, create a new markdown file at `prompts/library/release/generate-release-notes.md`. Populate this file with the initial YAML front matter, including `title`, `description`, and `phase: release`.",
            "dependencies": [],
            "details": "The file should be created in the `prompts/library/release/` directory. The YAML front matter should define the prompt's metadata. For example:\n---\ntitle: Generate Release Notes\ndescription: Creates a draft of release notes or a changelog from git history and guides through pre-release checks.\nphase: release\n---",
            "status": "pending",
            "testStrategy": "Verify the file `prompts/library/release/generate-release-notes.md` exists and contains valid YAML front matter with `phase: release`."
          },
          {
            "id": 2,
            "title": "Draft the initial prompt logic to gather raw data for release notes",
            "description": "Following the guidance from the task complexity report, write the first part of the prompt. This section should instruct the AI on how to gather the raw data needed for release notes, such as by requesting the user to provide `git log` output between two release tags.",
            "dependencies": [
              "16.1"
            ],
            "details": "Edit `prompts/library/release/generate-release-notes.md`. The prompt should ask the user to provide a list of commits or a git log. It should set the context for the AI to act as a release manager.",
            "status": "pending",
            "testStrategy": "Review the prompt to ensure it clearly asks for commit history or a similar raw input."
          },
          {
            "id": 3,
            "title": "Refine the prompt to format raw data into structured release notes",
            "description": "Enhance the prompt to process the raw input from the previous step. It should include instructions for categorizing commits (e.g., 'Features', 'Bug Fixes', 'Documentation') and formatting the output into a clean, markdown-formatted changelog.",
            "dependencies": [
              "16.2"
            ],
            "details": "Update `prompts/library/release/generate-release-notes.md`. Add instructions for the AI to parse the provided commit messages, group them by type (e.g., using conventional commit prefixes like 'feat:', 'fix:'), and generate structured markdown output.",
            "status": "pending",
            "testStrategy": "Execute the prompt with sample git log data and verify the output is a well-structured markdown changelog with categorized sections."
          },
          {
            "id": 4,
            "title": "Incorporate a pre-release checklist into the prompt",
            "description": "As recommended by the complexity report's expansion plan, add a final section to the prompt. This section should guide the user through a series of pre-release verification checks to ensure release readiness.",
            "dependencies": [
              "16.3"
            ],
            "details": "Append a new section to `prompts/library/release/generate-release-notes.md`. This part of the prompt should generate a checklist for the user, including items like: 'Have version numbers been bumped?', 'Have all automated tests passed?', 'Is the documentation updated?'.",
            "status": "pending",
            "testStrategy": "Run the prompt and confirm that its output includes a distinct and actionable pre-release checklist in addition to the release notes."
          },
          {
            "id": 5,
            "title": "Build and verify the /generate-release-notes prompt integration",
            "description": "Execute the project's build scripts to update the prompt catalog and documentation. Verify that the new `/generate-release-notes` prompt is correctly integrated into the system.",
            "dependencies": [
              "16.4"
            ],
            "details": "Run the `npm run build:prompts` command (or equivalent build script). After the script completes, inspect the generated `catalog.json` file to ensure it contains an entry for the new prompt. Also, check the `README.md` file to confirm the prompt appears in the 'Release' phase table.",
            "status": "pending",
            "testStrategy": "Confirm the existence of the new prompt's metadata in `catalog.json` and its visibility in the `README.md` tables after running the build process."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 17,
        "title": "Author Post-Release Hardening Prompts",
        "description": "Create at least one slash-command prompt for the 'post-release' phase of the development lifecycle.",
        "details": "As part of the 'Lifecycle Prompt Library', author a markdown prompt with YAML front matter indicating `phase: post-release`. The prompt could guide a user in creating a post-mortem document, identifying areas for technical debt repayment, or hardening a feature based on initial feedback.",
        "testStrategy": "Run the prompt and confirm its output is relevant to post-release activities. Verify its metadata is correctly added to `catalog.json` and the `README.md` tables after running the build scripts.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Scope and Goal for Post-Mortem Prompt",
            "description": "Brainstorm and select a specific post-release activity for the new prompt. Settle on creating a 'post-mortem analysis' prompt to guide users in documenting and learning from a release cycle.",
            "dependencies": [],
            "details": "Based on the parent task, decide on a concrete topic for the post-release prompt. The chosen topic is 'post-mortem analysis'. Define the prompt's primary goal: to help a user generate a structured post-mortem document by asking targeted questions about the release process, what went well, what didn't, and action items for improvement.",
            "status": "pending",
            "testStrategy": "Review the defined scope and goals to ensure they align with typical post-release activities and provide clear value to the user."
          },
          {
            "id": 2,
            "title": "Create 'conduct-post-mortem.md' and YAML Front Matter",
            "description": "Create the new markdown file for the post-mortem prompt in the correct directory and add the required YAML front matter.",
            "dependencies": [
              "17.1"
            ],
            "details": "Create a new file at `prompts/lifecycle/post-release/conduct-post-mortem.md`. In this file, add the YAML front matter with the following keys: `name` set to `/conduct-post-mortem`, `description` summarizing its purpose, and `phase` set to `post-release`. This establishes the prompt's metadata for the build system.",
            "status": "pending",
            "testStrategy": "Verify the file is created in the correct location (`prompts/lifecycle/post-release/`). Check that the YAML front matter is valid and contains the required `name`, `description`, and `phase` fields."
          },
          {
            "id": 3,
            "title": "Author System Prompt and Instructions for Post-Mortem Analysis",
            "description": "Write the system prompt and detailed instructions within 'conduct-post-mortem.md' that define the AI's role and the structure of the post-mortem.",
            "dependencies": [
              "17.2"
            ],
            "details": "In `prompts/lifecycle/post-release/conduct-post-mortem.md`, write the `# System Prompt` section. Define the AI's persona as an experienced project lead facilitating a blameless post-mortem. Under an `# Instructions` section, outline the steps the AI should take, such as asking for a summary of the release, what went well, what could be improved, and key learnings. The structure should encourage a comprehensive and constructive analysis.",
            "status": "pending",
            "testStrategy": "Read the system prompt and instructions to ensure they clearly guide the AI to generate a structured and helpful post-mortem document."
          },
          {
            "id": 4,
            "title": "Author User Prompt and Finalize 'conduct-post-mortem.md' Content",
            "description": "Complete the prompt by adding the user prompt section and refining the overall content for clarity and effectiveness.",
            "dependencies": [
              "17.3"
            ],
            "details": "In `prompts/lifecycle/post-release/conduct-post-mortem.md`, add the `# User Prompt` section. This section should include placeholders like `{{user_input}}` for the user to provide context about the project or release they want to analyze. Review and refine the entire prompt file for grammar, clarity, and flow.",
            "status": "pending",
            "testStrategy": "Execute the prompt with sample user input to confirm it generates a well-structured post-mortem document that follows the defined instructions."
          },
          {
            "id": 5,
            "title": "Verify Prompt Integration via Build Scripts",
            "description": "Run the project's build scripts and verify that the new '/conduct-post-mortem' prompt is correctly added to 'catalog.json' and the 'README.md' documentation.",
            "dependencies": [
              "17.4"
            ],
            "details": "Execute the build script responsible for updating the prompt catalog (e.g., `npm run build:prompts`). After the script completes, inspect the `catalog.json` file to ensure an entry for `/conduct-post-mortem` exists with the correct metadata. Also, check the `README.md` file to confirm the prompt is listed in the appropriate table for the 'post-release' phase.",
            "status": "pending",
            "testStrategy": "Confirm the existence and correctness of the new prompt's entry in `catalog.json`. Verify the `README.md` is updated and the link to the new prompt is not broken."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 18,
        "title": "Integrate and Test Full Metadata Workflow",
        "description": "Perform an end-to-end test of the metadata automation by editing a prompt, running validation, and rebuilding the catalog and README.",
        "details": "This task verifies that the core automation loop is robust. Make a change to a prompt's YAML front matter (e.g., edit its title). Run `npm run validate:metadata` and confirm it passes. Then run `npm run build:catalog`. Verify that `catalog.json` and the `README.md` tables reflect the change. This satisfies the acceptance criteria for 'Prompt Metadata Automation'.",
        "testStrategy": "Follow the steps in the details. Introduce an invalid change to a prompt's metadata and confirm `npm run validate:metadata` fails as expected. Fix the error and confirm both scripts succeed and update the artifacts correctly.",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test Branch for Metadata Workflow Verification",
            "description": "Set up a dedicated git branch to isolate the end-to-end testing of the metadata workflow, as recommended by the task expansion plan.",
            "dependencies": [],
            "details": "Create a new git branch named `test/metadata-workflow` from the current main branch. This ensures that the main branch remains clean during the test cycle and provides an isolated environment for making and reverting changes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Execute 'Happy Path' Test: Modify Metadata and Run Build",
            "description": "Perform a 'happy path' test by modifying a prompt's metadata, running validation and build scripts, and committing the successful changes.",
            "dependencies": [
              "18.1"
            ],
            "details": "On the `test/metadata-workflow` branch, edit the `title` in the YAML front matter of `prompts/1-analyze-issue.md`. Run `npm run validate:metadata` and confirm it passes. Then, run `npm run build:catalog` to update `catalog.json` and `README.md`. Commit all modified files to the branch.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Verify Artifacts After 'Happy Path' Build",
            "description": "Inspect `catalog.json` and `README.md` to confirm that the changes from the 'happy path' test were correctly applied.",
            "dependencies": [
              "18.2"
            ],
            "details": "After the successful build in the previous subtask, manually review `catalog.json` to ensure the `title` for prompt ID 1 has been updated. Also, check the prompt table within `README.md` to verify the new title is correctly reflected there.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Execute 'Failure Path' Test: Introduce Invalid Metadata",
            "description": "Test the validation logic by intentionally introducing an invalid change to a prompt's metadata and confirming that the validation script fails as expected.",
            "dependencies": [
              "18.3"
            ],
            "details": "On the `test/metadata-workflow` branch, edit `prompts/1-analyze-issue.md` again. Remove the `version` field from the YAML front matter. Run `npm run validate:metadata` and confirm that the script exits with a non-zero status code and logs a Zod validation error. Document the specific error message for verification. Do not commit this invalid change.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Revert Invalid Change, Re-run Build, and Finalize Test",
            "description": "Revert the invalid metadata change, re-run the build process to ensure the system returns to a valid state, and finalize the end-to-end test.",
            "dependencies": [
              "18.4"
            ],
            "details": "Revert the change made to `prompts/1-analyze-issue.md` in the previous step by restoring the `version` field. Re-run `npm run validate:metadata` to confirm it passes. This concludes the E2E test, leaving the branch in a clean state with the successful 'happy path' changes.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 19,
        "title": "Document MCP Evolution Readiness in README",
        "description": "Update the `README.md` to include the 'Future enhancements' section detailing the MCP evolution path.",
        "details": "Fulfill the 'MCP Evolution Readiness' requirement by adding a new section to the `README.md`. This section should describe the future-state architecture where the prompt pack is exposed as an MCP server with typed inputs and DocFetch event signals. Crucially, it must also specify the manual fallback paths (using slash commands) that remain functional if MCP is not available.",
        "testStrategy": "Review the `README.md` file after modification. Confirm the new 'Future enhancements' section exists and contains the required architectural details and a clear statement about the manual fallback workflow, as per the PRD's acceptance criteria.",
        "priority": "low",
        "dependencies": [
          18
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Add 'Future Enhancements' Section to README.md",
            "description": "Edit the `README.md` file to add a new level-two heading titled '## Future Enhancements'. This section will serve as the container for the MCP evolution documentation.",
            "dependencies": [],
            "details": "Locate the appropriate position in `README.md`, likely after the 'Usage' or 'Prompt Catalog' section, and insert the new markdown heading `## Future Enhancements` to create the designated area for the new content.",
            "status": "pending",
            "testStrategy": "Verify that the `README.md` file now contains the '## Future Enhancements' heading when viewed in a markdown renderer or text editor."
          },
          {
            "id": 2,
            "title": "Draft the Core MCP Server Vision",
            "description": "Under the new 'Future Enhancements' heading, write the primary description of the future-state architecture. This text should explain that the prompt pack will evolve to be exposed as a standalone MCP (Meta-prompt Communication Protocol) server.",
            "dependencies": [
              "19.1"
            ],
            "details": "Describe the high-level goal: transitioning from a static collection of prompts to a dynamic service. Mention that this aligns with a more robust, service-oriented architecture for interacting with prompts.",
            "status": "pending",
            "testStrategy": "Review the added text in `README.md` to confirm it clearly articulates the vision of the prompt pack becoming an MCP server."
          },
          {
            "id": 3,
            "title": "Detail Typed Inputs and DocFetch Event Signals",
            "description": "Expand on the MCP server vision by detailing its key technical features. Specifically, describe how the server will utilize typed inputs for prompts and emit DocFetch event signals.",
            "dependencies": [
              "19.2"
            ],
            "details": "Explain that typed inputs will provide a structured and validated way to invoke prompts, replacing free-form text. Describe how DocFetch event signals will allow the server to communicate its state and requirements regarding documentation freshness to other development tools.",
            "status": "pending",
            "testStrategy": "Check the `README.md` section to ensure the concepts of 'typed inputs' and 'DocFetch event signals' are present and explained in the context of the MCP server."
          },
          {
            "id": 4,
            "title": "Document the Manual Slash Command Fallback Path",
            "description": "Add a paragraph to the 'Future Enhancements' section that explicitly states the manual fallback mechanism. This must clarify that the current slash command interface will remain functional.",
            "dependencies": [
              "19.3"
            ],
            "details": "This is a critical requirement. The text must reassure users that even if the MCP server is not available or integrated, the prompts can still be used via the existing, simpler slash command workflow. This ensures backward compatibility and usability in diverse environments.",
            "status": "pending",
            "testStrategy": "Verify that the `README.md` contains a clear and unambiguous statement about the persistence of slash commands as a fallback option."
          },
          {
            "id": 5,
            "title": "Review and Finalize MCP Evolution Section",
            "description": "Perform a comprehensive review of the entire 'Future Enhancements' section. Check for clarity, consistency in tone with the rest of the `README.md`, and ensure all requirements from the parent task are fully met.",
            "dependencies": [
              "19.4"
            ],
            "details": "Read through the newly added section from the perspective of a new developer. Ensure the evolution path is easy to understand, the technical terms are used correctly, and the fallback mechanism is clearly highlighted. Correct any typos or grammatical errors.",
            "status": "pending",
            "testStrategy": "Have a peer or lead review the final `README.md` changes to confirm it meets the acceptance criteria of the parent task: the section exists, describes the MCP architecture, and specifies the manual fallback."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 20,
        "title": "Align export_task_list with Task Master backlog",
        "description": "Update the workflow MCP tool so it reads Task Master AI's tasks.json output, preserving dependencies, statuses, and subtask hierarchy for external agents.",
        "details": "Refactor `workflow/export_task_list` to parse `.taskmaster/tasks/tasks.json` using the existing Zod infrastructure. The tool should flatten subtasks into parent/child arrays, propagate Task Master status values, and include metadata tying each result back to the originating task ID. Update integration tests to cover Task Master sourced data and document the new behaviour in the MCP integration brief.",
        "testStrategy": "Add unit coverage for the new parser and extend `test/integration/tools.test.ts` to assert Task Master sourced results. Run `npm run test:tools` and `npm run test:state` to confirm no regressions.",
        "priority": "high",
        "dependencies": [
          8
        ],
        "status": "deprecated",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Task Master tasks parser",
            "description": "Create a loader that reads `.taskmaster/tasks/tasks.json`, validates structure with Zod, and returns normalized tasks including subtasks and dependencies.",
            "dependencies": [],
            "details": "Prefer a pure function that accepts a JSON object so tests can inject fixtures. Match Task Master status strings exactly (e.g., pending, in-progress, done).",
            "status": "pending",
            "testStrategy": "Unit test the parser with representative Task Master fixtures covering nested subtasks and dependency references."
          },
          {
            "id": 2,
            "title": "Wire parser into workflow/export_task_list",
            "description": "Replace the prompts metadata source with the new Task Master parser, ensuring structured results include parent-child relationships and status propagation.",
            "dependencies": [
              "20.1"
            ],
            "details": "Return payload should include `source: 'task-master'` and maintain the schema expected by existing clients (id, title, dependsOn, status).",
            "status": "pending",
            "testStrategy": "Extend integration tests in `test/integration/tools.test.ts` to assert the tool now reflects Task Master data."
          },
          {
            "id": 3,
            "title": "Document behaviour and update automation",
            "description": "Refresh MCP documentation (integration brief, README) and ensure `workflow/refresh_metadata` guidance covers Task Master sourced tasks.",
            "dependencies": [
              "20.2"
            ],
            "details": "Note any manual fallback commands and confirm DocFetchReport call-outs where applicable.",
            "status": "pending",
            "testStrategy": "Review docs for accuracy; no automated tests required."
          }
        ],
        "reason": "Superseded by PRDv2 delta on 2025-09-21",
        "superseded_by": [
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
        ]
      },
      {
        "id": 21,
        "title": "Provide CLI surface for prompts workflow",
        "description": "Create a Node.js CLI entry point that mirrors the MCP server capabilities (list prompts, refresh metadata, export task list, advance state) so users can operate without Task Master or an MCP client.",
        "details": "Implement a `bin/prompts.js` (or TypeScript equivalent) that wraps metadata validation, catalog rebuilds, prompt listing, and workflow exports. Ensure commands can backfill tasks from prompt metadata when Task Master is unavailable. Document usage in the integration brief and README, and wire npm scripts so the CLI is easily invoked (e.g., `npm run prompts -- list`).",
        "testStrategy": "Add unit tests for command handlers and run through an integration smoke test exercising list/refresh/export flows. Reuse existing test suites where possible.",
        "priority": "medium",
        "dependencies": [
          8,
          20
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold CLI command surface",
            "description": "Introduce a CLI scaffold (e.g., Commander or native argument parsing) with commands for list, refresh, and export.",
            "dependencies": [],
            "details": "Ensure stdout emits human-readable output while `--json` preserves structured payloads.",
            "status": "done",
            "testStrategy": "Unit test argument parsing and help output."
          },
          {
            "id": 2,
            "title": "Wire workflow commands to existing scripts",
            "description": "Hook CLI commands into metadata validation, catalog rebuild, and export_task_list backfill logic.",
            "dependencies": [
              "21.1"
            ],
            "details": "Reuse the same utilities that power MCP tools to avoid drift.",
            "status": "done",
            "testStrategy": "Integration test the CLI using a temporary workspace to ensure scripts run and outputs are generated."
          },
          {
            "id": 3,
            "title": "Document CLI usage and fallback flows",
            "description": "Update the integration brief and README to show CLI-based workflows, including Task Master independent flows.",
            "dependencies": [
              "21.2"
            ],
            "details": "Include examples for backfilling tasks and refreshing metadata without MCP clients.",
            "status": "done",
            "testStrategy": "Peer review documentation for accuracy."
          }
        ]
      },
      {
        "id": 22,
        "title": "Document CLI Distribution and Usage Lifecycle",
        "description": "Create comprehensive documentation in the README.md for publishing the Node.js CLI to npm and for end-user installation and usage, based on the CLI created in Task 21.",
        "details": "A new section, titled 'CLI Distribution and Usage', should be added to the main `README.md`. This section must provide clear guidance for developers on the entire lifecycle of the CLI tool.\n\n1.  **Prepare for Publishing:**\n    *   Confirm the `bin` field in `package.json` correctly maps a command name (e.g., `prompts-cli`) to the compiled JavaScript entry point (e.g., `dist/bin/prompts.js`).\n    *   Ensure the main CLI source file (likely `src/bin/prompts.ts` or similar from Task 21) includes a shebang at the very top: `#!/usr/bin/env node`.\n    *   Add or verify the `files` array in `package.json` to include `\"dist\"`, `\"README.md\"`, and other necessary assets, ensuring no source or test files are published.\n\n2.  **Local Development Workflow:**\n    *   Document the `npm link` process for testing the CLI globally without publishing. The steps are: `npm run build` (to compile TypeScript), followed by `npm link`.\n    *   Provide an example of how to test the linked command (e.g., `prompts-cli list`).\n    *   Include instructions for `npm unlink` to remove the local symlink.\n\n3.  **Publishing to npm:**\n    *   Outline the standard procedure: `npm login`, version bumping with `npm version <patch|minor|major>`, and publishing with `npm publish`.\n    *   Recommend adding a `prepublishOnly` script to `package.json` that runs the build and test scripts (`npm run test && npm run build`) to prevent accidental publishing of broken code.\n\n4.  **End-User Installation and Usage:**\n    *   Provide the command for global installation: `npm install -g <package-name>`.\n    *   Show a basic usage example, referencing the commands implemented in Task 21, such as `prompts-cli list` and `prompts-cli refresh`.",
        "testStrategy": "1. Review the `README.md` file to confirm the new 'CLI Distribution and Usage' section is present and covers all points from the implementation details.\n2. Manually execute the documented local development workflow: run `npm run build`, then `npm link`. Open a new terminal session and execute one of the CLI commands (e.g., `prompts-cli list`). Confirm it works as expected. Run `npm unlink` and verify the command is no longer available.\n3. Inspect `package.json` to ensure the `bin`, `files`, and recommended `prepublishOnly` script are correctly configured.\n4. Check the primary CLI source file (e.g., `src/bin/prompts.ts`) to verify the `#!/usr/bin/env node` shebang is present.",
        "status": "done",
        "dependencies": [
          21
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Capture local CLI build smoke test",
            "description": "Write the guidance for building the CLI (`npm run build` / `node dist/cli/main.js`) and verifying command output locally before any linking. Definition of done: instructions mention rebuilding after changes and checking the command via Node directly.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 22
          },
          {
            "id": 2,
            "title": "Document npm link workflow for local testing",
            "description": "Detail the two-step `npm link` process (global link from CLI project, then link into a sample consumer) plus cleanup commands. Definition of done: guidance covers shortcut usage, unlinking, and cautions about re-running after rebuilds.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 22
          },
          {
            "id": 3,
            "title": "Outline publish preparation checklist",
            "description": "Capture the pre-publish steps: package.json metadata review, `npm version` bump, `npm pack` inspection, optional dry run, and scoped package access flags. Definition of done: checklist references provenance/OTP considerations and stresses not to overwrite versions.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 22
          },
          {
            "id": 4,
            "title": "Describe global install and maintenance guidance",
            "description": "Add instructions for publishing (`npm publish`), global installation (`npm install -g`), updates, and uninstall paths for end users. Definition of done: notes include verifying README on npm, recommending `npm update -g`, and documenting uninstall commands.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 22
          }
        ]
      },
      {
        "id": 23,
        "title": "Execute and Verify CLI Distribution Workflow",
        "description": "Execute the end-to-end CLI distribution workflow documented in Task 22, performing all steps locally to validate the instructions and ensure the package is ready for a real npm publish.",
        "details": "This task involves a full dry-run of the CLI publishing and installation lifecycle to prove the documentation from Task 22 is accurate and the package is configured correctly. Follow the 'CLI Distribution and Usage' section of README.md.\n\n1.  **Build the Project:** Start by ensuring the project compiles correctly. Run `npm run build`. This should create the distributable JavaScript files in the `dist/` directory, including the CLI entry point at `dist/bin/prompts.js`.\n\n2.  **Local Development Simulation (`npm link`):**\n    *   Execute `npm link` in the project root. This will create a global symlink to your local project, making the `prompts-cli` command (as defined in `package.json`'s `bin` field) available system-wide.\n    *   Open a new terminal session (to ensure the path is updated) and run `prompts-cli --version` and `prompts-cli list`. Confirm they execute without errors.\n    *   When finished, run `npm unlink` to remove the symlink.\n\n3.  **Package Validation (`npm pack`):**\n    *   Run `npm pack`. This command creates a gzipped tarball (`.tgz`) just as it would for publishing. This is a critical step to verify that the `files` array in `package.json` is correct and that no sensitive or unnecessary files (like `src/` or `.env`) are included.\n\n4.  **Publish Simulation (`npm publish --dry-run`):**\n    *   Execute `npm publish --dry-run`. This will perform all the pre-publish checks and show a summary of the package contents that would be uploaded to the npm registry, without actually publishing.\n\n5.  **Local Installation Test:**\n    *   Create a new empty directory outside of the project (e.g., `~/cli-test`).\n    *   From within that new directory, install the package from the tarball created in step 3: `npm install /path/to/project/your-package-name-1.0.0.tgz`.\n    *   Verify you can run the command via `./node_modules/.bin/prompts-cli`.\n\n6.  **Global Installation Test:**\n    *   Run `npm install -g /path/to/project/your-package-name-1.0.0.tgz` to simulate a global install for an end-user.\n    *   Open a new terminal and execute `prompts-cli list` to confirm it works globally.\n    *   Clean up by running `npm uninstall -g <package-name>`.\n\n7.  **Documentation Feedback:** If any of these steps fail or the instructions in `README.md` are unclear, update the documentation as part of this task.",
        "testStrategy": "The successful completion of this task is the test itself. To verify:\n1. Confirm that `npm link` followed by running `prompts-cli list` in a new shell session executes successfully.\n2. Inspect the contents of the tarball generated by `npm pack` to ensure it only contains production files (`dist`, `package.json`, `README.md`, etc.) and not source files (`src`).\n3. Review the output of `npm publish --dry-run` to confirm there are no errors and the file list is correct.\n4. Provide evidence that installing the package tarball (both locally to a new project and globally with `-g`) results in a functioning CLI.\n5. If any documentation changes were required, they must be included in the final pull request. If no changes were needed, a comment confirming the successful validation of all documented steps is required.",
        "status": "done",
        "dependencies": [
          21,
          22
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Run local build and direct CLI smoke tests",
            "description": "Execute `npm run build` and invoke the CLI via `node dist/cli/main.js` for representative commands to confirm local output matches expectations. Capture results and note any issues.",
            "details": "<info added on 2025-09-20T19:37:08.621Z>\n2025-09-20T19:36:58Z — Performed a prerequisite check to validate the build artifact before linking.\n- Ran `npm run build` (`tsc`), which completed successfully.\n- Directly invoked the compiled CLI entry point `dist/cli/main.js` to smoke test the primary commands (`--help`, `list`, `export`).\n- All commands returned the expected output, confirming the local build is valid and ready for the `npm link` test.\n</info added on 2025-09-20T19:37:08.621Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          },
          {
            "id": 2,
            "title": "Validate npm link workflow",
            "description": "Use `npm link` to create a global symlink for the CLI, link it into a sample consumer project, exercise key commands, and then clean up with `npm unlink`. Record commands and outcomes.",
            "details": "<info added on 2025-09-20T19:39:18.009Z>\nValidated the `npm link` workflow using a local prefix to avoid global permission issues: `npm_config_prefix=\"$(pwd)/tmp/npm-global\" npm link`.\n\nTo test, a temporary consumer app was created and the package was linked into it (`npm link prompts`). The CLI was successfully exercised via the temporary bin path: `tmp/npm-global/bin/prompts list --json`. The setup was then torn down with `npm unlink` in both the consumer and source directories.\n\n**Note:** A key finding is that the linked CLI currently must be run from the project's root directory for its resource files (e.g., `prompts.meta.yaml`) to resolve correctly, as it uses paths relative to the current working directory rather than the script's own location.\n</info added on 2025-09-20T19:39:18.009Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          },
          {
            "id": 3,
            "title": "Perform publish prep dry run",
            "description": "Review package metadata, bump a prerelease version (or note skipped), run `npm pack` to inspect the tarball, and document provenance/OTP requirements without publishing. Summarize readiness in notes.",
            "details": "<info added on 2025-09-20T19:40:19.038Z>\n2025-09-20T19:42:21Z — Performed a dry run of the packaging process using `npm pack --dry-run`. The command failed as expected because the `package.json` file is currently marked `\"private\": true` and lacks a `version` property.\n\nTo prepare for publishing, the following changes are required in `package.json`:\n1.  Set `\"private\": false`.\n2.  Initialize a semantic version, for example by running `npm version 0.1.0 --no-git-tag-version`.\n\nAlso noted that the final publish command will be `npm publish --provenance --access public`, which will require OTP authentication. No package tarball was generated.\n</info added on 2025-09-20T19:40:19.038Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          },
          {
            "id": 4,
            "title": "Plan global publish and support steps",
            "description": "Outline the exact `npm publish` command (with flags), post-publish verification on npmjs.com, global installation instructions for users, and update/rollback procedures. Hold actual publish until approvals allow.",
            "details": "<info added on 2025-09-20T19:41:04.909Z>\n[]\n</info added on 2025-09-20T19:41:04.909Z>\n<info added on 2025-09-20T19:41:29.593Z>\nDrafted release plan:\n\n-   **Pre-Publish:**\n    -   Update `package.json`: Add a `version` field (e.g., `1.0.0`) and remove the `\"private\": true` field to allow publishing.\n-   **Publish Command:**\n    -   From the repository root, execute: `npm publish --provenance --access public`\n    -   Note: This will require a One-Time Password (OTP) from an authenticator app.\n-   **Post-Publish Verification:**\n    -   Check the package page on npmjs.com (e.g., `https://www.npmjs.com/package/prompts`) to confirm the README is rendered correctly and package metadata (version, license, etc.) is accurate.\n    -   Verify the published version from the command line: `npm view prompts version`.\n    -   If necessary, add additional distribution tags (e.g., `npm dist-tag add prompts@<version> beta`).\n-   **User Installation & Usage Guidance:**\n    -   **Install:** `npm install -g prompts`\n    -   **Update:** `npm update -g prompts`\n    -   **Uninstall:** `npm uninstall -g prompts`\n-   **Support and Rollback Plan:**\n    -   **Standard Rollback:** To revert users to a previous stable version, use `npm dist-tag add prompts@<stable-version> latest`.\n    -   **Emergency Unpublish:** Only if a critical issue is found within 24 hours of publishing, use `npm unpublish prompts@<bad-version>`. This is a destructive action and should be avoided if possible due to its impact on the ecosystem.\n\nThis plan has been captured for execution upon approval. No publish action has been taken yet.\n</info added on 2025-09-20T19:41:29.593Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          }
        ]
      },
      {
        "id": 24,
        "title": "Finalize npm Publish Readiness",
        "description": "Prepare the package for its initial npm release by updating package.json, removing the private flag, setting a version, and running final pre-flight checks.",
        "details": "This task involves the final steps to make the package publishable to the public npm registry. It builds upon the dry-run validation performed in Task 23.\n\n1. **Update `package.json`:**\n   - Open `package.json`.\n   - Remove the line `\"private\": true`.\n   - Set the `\"version\"` field to `\"0.1.0\"` for the initial release.\n   - Briefly review other metadata fields like `description`, `repository`, `license`, and `keywords` for accuracy.\n\n2. **Clean and Rebuild:**\n   - Ensure a clean build environment by removing previous artifacts: `rm -rf dist/` and `rm -f *.tgz`.\n   - Re-run the production build process: `npm run build`.\n\n3. **Final Packaging Validation:**\n   - Generate the final package tarball locally: `npm pack`.\n   - Inspect the contents of the generated `.tgz` file (e.g., `tar -tvf prompts-cli-0.1.0.tgz`) to confirm it only contains production-necessary files (`package.json`, `README.md`, the `dist/` directory, etc.) and excludes source code (`src/`) or development configurations (`tsconfig.json`).\n\n4. **Pre-Publish Check:**\n   - Authenticate with npm using `npm login`.\n   - Perform a final check using npm's dry-run feature: `npm publish --dry-run`. This will simulate the publish process and report the exact files being included without actually releasing the package.",
        "testStrategy": "1. After editing `package.json`, use `grep` or `cat` to confirm that the `\"private\": true` line is removed and the `\"version\"` is set to `\"0.1.0\"`.\n2. Run `npm pack` and inspect the resulting tarball's contents using `tar -tvf *.tgz`. Verify that the file list matches expectations for a production package (no `src` folder, no `tsconfig.json`).\n3. Execute `npm publish --dry-run`. The test passes if the command completes successfully and the output lists the expected package contents without any errors or warnings about missing files or authentication issues.",
        "status": "done",
        "dependencies": [
          23
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update package.json for publishability",
            "description": "Set \"private\": false and add an initial version (e.g., 0.1.0) using npm version or direct edit, keeping semver formatting.",
            "details": "<info added on 2025-09-20T20:18:14.080Z>\n2025-09-20T19:59:41Z — Ran `npm version 0.1.0 --no-git-tag-version` after confirming `package.json` already had `\"private\": false`. File now includes `\"version\": \"0.1.0\"`, making the package publishable.\n</info added on 2025-09-20T20:18:14.080Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 24
          },
          {
            "id": 2,
            "title": "Re-run packaging validation",
            "description": "Execute `npm pack --dry-run` (or `npm pack`) and inspect output tarball contents to ensure readiness.",
            "details": "<info added on 2025-09-20T20:19:12.168Z>\n2025-09-20T20:01:55Z — Executed `npm pack --dry-run` to validate package contents. The command succeeded, but the output revealed that development-only directories (`.taskmaster/`, `tmp/`) are being included in the tarball.\n\nTo ensure a clean package, the `package.json` file must be updated to explicitly define what gets published. The recommended approach is to add a `\"files\"` array. Based on the project's build process which outputs to a `dist/` directory, this array should contain entries like `\"dist\"`, `\"README.md\"`, and the `LICENSE` file if present. This 'allow-list' approach is preferred over `.npmignore` for this project.\n</info added on 2025-09-20T20:19:12.168Z>\n<info added on 2025-09-20T20:20:27.675Z>\n2025-09-20T20:05:10Z — Added a `files` allow-list to `package.json` and re-ran `npm pack --dry-run`. The tarball now contains only the `dist/` output plus necessary metadata/resources (35 files, ~37 kB). Package is clean for publishing.\n</info added on 2025-09-20T20:20:27.675Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 24
          },
          {
            "id": 3,
            "title": "Prepare publish runbook update",
            "description": "Revise the release checklist with final commands, note provenance/OTP steps, and capture next actions once approvals land.",
            "details": "<info added on 2025-09-20T20:21:32.809Z>\nUpdated `README.md` to include a new \"Publishing to npm\" section. This checklist acts as a release runbook, detailing the manual steps for publishing the package. The new section covers:\n- Pre-release: Bumping the version in `package.json`.\n- Validation: Running a clean `npm pack --dry-run` to verify package contents.\n- Publishing: Using the command `npm publish --provenance --access public` for a secure, public release.\n- Post-release: Verifying the package on the npm registry.\n- Contingency: High-level guidance for rolling back a release.\n</info added on 2025-09-20T20:21:32.809Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 24
          }
        ]
      },
      {
        "id": 25,
        "title": "Project Initialization and Scaffolding",
        "description": "Set up a new Node.js project using pnpm or npm. Configure TypeScript, Jest for testing, and establish the basic directory structure required by the PRD.",
        "details": "Initialize a `package.json`. Install `typescript`, `ts-node`, `@types/node`, `jest`, and `ts-jest` as dev dependencies. Create a `tsconfig.json` file with modern settings. Create the initial directory structure: `src/`, `schemas/`, `bin/`, `packages/`, `tests/`.",
        "testStrategy": "Verify that `pnpm test` runs successfully and that a simple 'hello world' TypeScript file in `src/` can be compiled and executed.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 26,
        "title": "Define Canonical Task JSON Schema",
        "description": "Create the canonical `task.json` schema file that acts as a superset of the Task-Master task definition, ensuring type safety and forward compatibility.",
        "details": "Create the file `schemas/task.json`. It must include all Task-Master fields: `id, title, description, status, dependencies, priority, details, testStrategy, subtasks`. Add the new optional fields: `labels, metadata, evidence, artifacts`.",
        "testStrategy": "Create a test that validates a canonical Task-Master task object against the schema, ensuring it passes. Create another test with the new optional fields to ensure it also passes. A test with a missing required field should fail.",
        "priority": "high",
        "dependencies": [
          25
        ],
        "status": "done",
        "subtasks": [],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 27,
        "title": "Implement Task-Master Ingest Adapter",
        "description": "Create a module to read and parse a Task-Master `tasks.json` file, validate it against the canonical schema, and normalize it into an array of `PromptsTask` objects.",
        "details": "Create `src/adapters/taskmaster/ingest.ts`. The main function will take a file path, read the JSON content, validate it using the schema from `schemas/task.json`, and map the data to an internal `PromptsTask[]` type. It should also handle mapping non-canonical status values and report them.",
        "testStrategy": "BDD: Given a valid Task-Master `tasks.json`, when the adapter runs, then it produces `PromptsTask[]` with identical core fields and records any non-canonical statuses in a mapping report. Use golden files for testing.",
        "priority": "high",
        "dependencies": [
          26
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold ingest adapter and shared task types",
            "description": "Add the AJV runtime dependency (`pnpm add ajv`) and create `src/adapters/taskmaster/ingest.ts` plus `src/types/prompts-task.ts` exporting the superset `PromptsTask` interface shared across the state engine.",
            "dependencies": [],
            "details": "Install `ajv` in the root workspace. In `src/types/prompts-task.ts`, define the TypeScript interface mirroring `schemas/task.json`, including optional `labels`, `metadata`, `evidence`, and `artifacts`. Export a `StatusAliasMap` stub for later normalization. Stub the ingest module with a named async function signature `ingestTasks(filePath: string): Promise<PromptsTask[]>`.",
            "status": "done",
            "testStrategy": "Run `pnpm exec tsc --noEmit` to confirm the new types compile and the ingest module exports the planned symbols."
          },
          {
            "id": 2,
            "title": "Implement ingest pipeline with AJV validation and status normalization",
            "description": "Read a Task-Master `tasks.json`, validate it against `schemas/task.json` using AJV, and map entries into `PromptsTask[]` while normalizing status aliases and collecting a mapping report.",
            "dependencies": [
              "27.1"
            ],
            "details": "In `src/adapters/taskmaster/ingest.ts`, load and compile the schema via AJV, throw on validation failures with aggregated errors, and map tasks to the `PromptsTask` shape. Normalize statuses (e.g., `todo` -> `pending`) using the alias map and attach any non-canonical statuses to a side-channel report exported from the module.",
            "status": "done",
            "testStrategy": "Create fixtures under `test/fixtures/taskmaster/` for valid and invalid inputs. Run targeted unit tests (next subtask) to assert validation failures and normalization outcomes."
          },
          {
            "id": 3,
            "title": "Write Jest coverage for ingest adapter",
            "description": "Add `tests/adapters/ingest.test.ts` verifying success, validation failure, and status normalization scenarios using golden fixtures.",
            "dependencies": [
              "27.2"
            ],
            "details": "Configure Jest to load TypeScript via `ts-jest`. Mock filesystem reads with `fs.promises`. Assert that invalid payloads produce descriptive errors and that the normalization report lists each remapped status.",
            "status": "done",
            "testStrategy": "Run `pnpm exec jest tests/adapters/ingest.test.ts --runInBand` and confirm all cases pass with deterministic snapshots where applicable."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 28,
        "title": "Implement State Engine: Readiness and 'next' Logic",
        "description": "Develop the core state engine logic to determine task readiness and select the next task based on the specified ordering rules.",
        "details": "Create `src/state/graph.ts`. Implement `computeReadiness(tasks)` to identify tasks where all dependencies are 'done'. Implement `next(tasks)` to filter for ready tasks and sort them by priority (desc), then dependency count (desc), then ID (asc), returning the top one.",
        "testStrategy": "BDD: Given a task graph, when `next()` runs, then it returns the highest-priority ready task per tie-break rules and never returns a task with unsatisfied dependencies. Test cases should include multiple ready tasks with different priorities and tie-breaker scenarios.",
        "priority": "high",
        "dependencies": [
          27
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create state graph module scaffold",
            "description": "Introduce `src/state/graph.ts` exporting placeholders for `computeReadiness` and `next`, along with helper types and priority mappings.",
            "dependencies": [],
            "details": "Define enums or lookup maps translating textual priorities to numeric weights. Add utility functions for dependency lookup (`buildStatusMap`, `countDependents`). Ensure the module exports the helpers for reuse in tests.",
            "status": "done",
            "testStrategy": "Run `pnpm exec tsc --noEmit` to ensure the scaffold compiles and exports the expected signatures."
          },
          {
            "id": 2,
            "title": "Implement computeReadiness with unit tests",
            "description": "Filter tasks to those whose dependencies are satisfied (done or deprecated) using a Map-based lookup for O(n) evaluation.",
            "dependencies": [
              "28.1"
            ],
            "details": "Implement `computeReadiness(tasks: PromptsTask[]): PromptsTask[]` and add tests in `test/state/graph/computeReadiness.test.ts` covering tasks with missing dependencies, blocked tasks, and alias statuses.",
            "status": "done",
            "testStrategy": "Run `pnpm exec jest test/state/graph/computeReadiness.test.ts`. Ensure coverage includes edge cases such as self-dependencies and circular detection returning an empty set."
          },
          {
            "id": 3,
            "title": "Implement next() tie-break sorting with coverage",
            "description": "Implement `next(tasks)` by sorting ready tasks by priority, dependent count, then ID, and add Jest tests for the multi-key ordering.",
            "dependencies": [
              "28.2"
            ],
            "details": "Leverage the helper utilities to compute dependent counts. Write tests in `test/state/graph/next.test.ts` verifying deterministic ordering across identical priorities and dependency fan-out, plus the null case when no tasks ready.",
            "status": "done",
            "testStrategy": "Run `pnpm exec jest test/state/graph/next.test.ts --runInBand` to verify ordering logic."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 29,
        "title": "Implement State Engine: Status Update Logic",
        "description": "Create a pure function to handle advancing a task's status within the in-memory state.",
        "details": "Create `src/state/update.ts`. Implement `advance(tasks, id, newStatus)` which takes the current task array, a task ID, and a new status, and returns a new task array with the specified task's status updated. The function should be pure and not mutate the input array.",
        "testStrategy": "Write unit tests to confirm that calling `advance` with a valid ID and status returns a new array with the correct task updated. Test that the original array remains unchanged.",
        "priority": "high",
        "dependencies": [
          28
        ],
        "status": "done",
        "subtasks": [],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 30,
        "title": "Implement MCP Stdio Server",
        "description": "Expose the state engine's capabilities over stdio using the Model Context Protocol (MCP) for consumption by external agentic clients.",
        "details": "Create `src/mcp/server.ts`. This Node.js script will listen for line-delimited JSON requests on stdin and write JSON responses to stdout. Implement handlers for `next_task`, `set_task_status`, `get_task`, `list_tasks`, and `graph_export`. The `set_task_status` handler must respect a write-mode flag to control persistence.",
        "testStrategy": "BDD: Given an MCP client connected over stdio, when it calls `next_task`, then the server returns a single task payload. When it calls `set_task_status` without write mode, the underlying file is not changed.",
        "priority": "medium",
        "dependencies": [
          29
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap stdio MCP server runtime",
            "description": "Create `src/mcp/server.ts` with a readline loop over stdin/stdout, structured logging hooks, and graceful shutdown handlers.",
            "dependencies": [],
            "details": "Set up the readline interface, ensure JSON parse errors are caught and reported, and wire SIGINT/SIGTERM to close the interface. Export a `startServer(options)` function for reuse in tests.",
            "status": "done",
            "testStrategy": "Run the server manually with `node src/mcp/server.ts` piping a sample line to confirm it echoes a well-formed error on invalid JSON."
          },
          {
            "id": 2,
            "title": "Load tasks and manage in-memory state",
            "description": "Integrate the ingest adapter to load tasks.json at startup, cache the array, and expose helpers to persist updated state when writes are enabled.",
            "dependencies": [
              "30.1",
              "27.3"
            ],
            "details": "Introduce configuration for the tasks file path and write-mode flag. Ensure state persists via atomic write (temporary file rename) to avoid corruption.",
            "status": "done",
            "testStrategy": "Add unit tests in `test/mcp/server/state.test.ts` mocking filesystem operations to verify load and flush behavior."
          },
          {
            "id": 3,
            "title": "Implement read-only MCP handlers",
            "description": "Handle `next_task`, `get_task`, `list_tasks`, and `graph_export` requests by dispatching to state engine helpers and writing JSON-RPC responses.",
            "dependencies": [
              "30.2",
              "28.3"
            ],
            "details": "Create a handler map keyed by method names. Ensure each response includes `jsonrpc`, `id`, and `result` fields and that errors return `error` objects with codes.",
            "status": "done",
            "testStrategy": "Add integration tests in `test/mcp/server/handlers.test.ts` sending mock JSON lines and asserting the parsed responses."
          },
          {
            "id": 4,
            "title": "Support set_task_status with persistence and regression tests",
            "description": "Implement the write-capable handler that applies `advance` updates, persists the task list when write mode is enabled, and returns confirmation payloads.",
            "dependencies": [
              "30.3",
              "29"
            ],
            "details": "Reuse the state update utilities to mutate in-memory tasks. Guard writes when `options.writeEnabled` is false and return a warning message instead of persisting.",
            "status": "done",
            "testStrategy": "Add a Jest integration test covering both write-enabled and read-only modes, asserting file writes and response structure."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 31,
        "title": "Implement Thin CLI with Commander.js",
        "description": "Build a command-line interface to provide fast, local access to the core features for developers and CI/CD pipelines.",
        "details": "Create `bin/prompts` and use the `commander` library. Implement subcommands: `ingest`, `next`, `advance <id> <status>`, `graph --format dot|json`, and `status`. These commands will call the pure functions from the adapter and state engine. Default output should be machine-readable JSON.",
        "testStrategy": "BDD: Given a repository containing `tasks.json`, when `prompts next` runs, then it prints a single JSON object for the next task to stdout and exits with code 0. Test graph command for both `dot` and `json` formats.",
        "priority": "medium",
        "dependencies": [
          29
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CLI entry point with Commander",
            "description": "Add `src/bin/cli.ts` with shebang, initialize `commander` program, and wire common options (verbose, task file path).",
            "dependencies": [],
            "details": "Ensure the entry compiles to `dist/bin/cli.js` and update `package.json` bin mapping. Export a `runCli(processArgs)` function for testability.",
            "status": "done",
            "testStrategy": "Run `node src/bin/cli.ts --help` to verify command registration and help output."
          },
          {
            "id": 2,
            "title": "Implement read commands (next, status, list)",
            "description": "Wire `next`, `status`, and `list` commands to call ingest/state helpers and pretty-print JSON results.",
            "dependencies": [
              "31.1",
              "27.3",
              "28.3"
            ],
            "details": "Ensure commands accept `--pretty` flag and handle empty results. Share option parsing logic across commands.",
            "status": "done",
            "testStrategy": "Add CLI smoke tests using `execa` snapshots to assert command output for sample tasks fixture."
          },
          {
            "id": 3,
            "title": "Implement advance command with persistence",
            "description": "Add `advance <id> <status>` command that updates tasks via `advance()` and writes results back to the source file atomically.",
            "dependencies": [
              "31.2",
              "29"
            ],
            "details": "Validate status inputs, surface normalization warnings from the ingest adapter, and honor a `--dry-run` flag to skip writes during CI.",
            "status": "done",
            "testStrategy": "Add tests in `test/cli/advance.test.ts` stubbing filesystem writes to confirm dry-run vs write behavior."
          },
          {
            "id": 4,
            "title": "Add graph export command with DOT support",
            "description": "Implement `graph` command accepting `--format json|dot`, building DOT output for dependencies when requested.",
            "dependencies": [
              "31.2"
            ],
            "details": "Create a formatter utility that converts the task list into DOT syntax. Warn when Graphviz is unavailable but still emit DOT text.",
            "status": "done",
            "testStrategy": "Add unit tests for the formatter ensuring deterministic node and edge ordering."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 32,
        "title": "Create Mastra Tools Package for AI SDK",
        "description": "Package the core logic as a set of tools compatible with the Vercel AI SDK to enable integration with Mastra agents.",
        "details": "In the `packages/` directory, create a new package named `prompts-tools`. This package will export tool handlers (e.g., `next_task`, `set_task_status`) that wrap the core state engine functions in a format consumable by `generateObject` or similar AI SDK functions.",
        "testStrategy": "BDD: Given a Mastra agent configured with `prompts-tools`, when the agent plans a step requiring the next task, then it successfully invokes the `next_task` tool and incorporates the result into its plan.",
        "priority": "medium",
        "dependencies": [
          29
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up packages/prompts-tools workspace package",
            "description": "Create `packages/prompts-tools` with package.json, tsconfig, and entry point scaffolding; update pnpm workspace configuration.",
            "dependencies": [],
            "details": "Define build script using `tsup` or `tsc`. Ensure package references shared state engine code without circular deps.",
            "status": "pending",
            "testStrategy": "Run `pnpm install` and `pnpm exec tsc --build packages/prompts-tools` to verify dependency graph."
          },
          {
            "id": 2,
            "title": "Implement Mastra tool definitions",
            "description": "Expose `next_task`, `set_task_status`, and `graph_export` tool definitions using Vercel AI SDK helpers and Zod parameter schemas.",
            "dependencies": [
              "32.1",
              "28.3",
              "29"
            ],
            "details": "Define schema objects aligning with MCP handlers, ensuring responses serialize cleanly for the AI SDK.",
            "status": "pending",
            "testStrategy": "Add unit tests verifying schema validation and handler invocation using mocked state engine functions."
          },
          {
            "id": 3,
            "title": "Publish package build outputs and docs",
            "description": "Add README and build artifacts ensuring package compiles to ESM/CJS targets required by Mastra, plus integration smoke test.",
            "dependencies": [
              "32.2"
            ],
            "details": "Configure build outputs under `dist/` and ensure `package.json` exports field is set. Document installation and usage.",
            "status": "pending",
            "testStrategy": "Run `pnpm test --filter prompts-tools` (once defined) and a sample script that loads the built package to ensure runtime compatibility."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 33,
        "title": "Implement Provider Presets and Fallback Logic",
        "description": "Configure zero-cost and local-first provider presets for the Mastra integration, ensuring the system is functional out-of-the-box.",
        "details": "Create provider configuration modules like `src/providers/ollama.ts` and `src/providers/geminiCli.ts`. Implement a runtime factory that checks for provider availability (e.g., Ollama server running) and selects one, defaulting to Ollama. If no primary providers are available, it should use a stub provider and log a warning.",
        "testStrategy": "BDD: Given no network access and Ollama installed, the agent initializes with the Ollama preset. Given Ollama is unavailable but Gemini CLI is, it selects Gemini. Given neither is available, it selects the stub provider and logs a warning.",
        "priority": "medium",
        "dependencies": [
          32
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define provider interface and shared types",
            "description": "Create `src/providers/interface.ts` with the `LLMProvider` contract and shared error classes for availability checks.",
            "dependencies": [],
            "details": "Include methods for `ping()` and `generate(prompt)`. Export specific error types for missing dependencies and offline providers.",
            "status": "pending",
            "testStrategy": "Add unit tests ensuring custom errors serialize expected metadata and that interface typings compile."
          },
          {
            "id": 2,
            "title": "Implement Ollama provider with health probe",
            "description": "Create `src/providers/ollama.ts` implementing the interface and verifying local Ollama availability via HTTP health endpoint.",
            "dependencies": [
              "33.1"
            ],
            "details": "Use `node-fetch` or native fetch to query `http://localhost:11434/api/tags`. Provide descriptive errors when not reachable.",
            "status": "pending",
            "testStrategy": "Add tests mocking fetch responses for success and failure, ensuring fallback errors bubble appropriately."
          },
          {
            "id": 3,
            "title": "Implement Gemini CLI and stub providers",
            "description": "Add `src/providers/geminiCli.ts` detecting CLI availability via `which`/`spawn`, plus `src/providers/stub.ts` returning warning responses.",
            "dependencies": [
              "33.1"
            ],
            "details": "Ensure Gemini provider reads API key configuration where applicable and that stub provider logs actionable warnings.",
            "status": "pending",
            "testStrategy": "Write unit tests stubbing child_process results to cover present/absent CLI scenarios and verifying stub outputs."
          },
          {
            "id": 4,
            "title": "Build provider factory with fallback chain",
            "description": "Implement `src/providers/factory.ts` selecting providers in priority order (Ollama -> Gemini -> stub) with robust error handling.",
            "dependencies": [
              "33.2",
              "33.3"
            ],
            "details": "Expose async `getProvider()` returning the first healthy provider and capturing telemetry for diagnostics.",
            "status": "pending",
            "testStrategy": "Add integration tests simulating various availability combinations to ensure expected provider selection."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 34,
        "title": "Document Client Configuration for Codex and Gemini",
        "description": "Create clear documentation with copy-pasteable snippets to help users configure their agentic clients (Codex, Gemini) to use the MCP stdio server.",
        "details": "Create a `docs/client_setup.md` file. Provide TOML and JSON configuration examples for registering the MCP server. Include placeholder paths and specific examples for Windows+WSL using the `/c/Users/user/...` format.",
        "testStrategy": "BDD: Given a user copies the provided snippets and updates the path, when the client starts, then the client lists the `prompts` server as available and tool calls succeed over stdio. This will be verified by manual testing following the guide.",
        "priority": "low",
        "dependencies": [
          30
        ],
        "status": "pending",
        "subtasks": [],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 35,
        "title": "Implement Optional Artifact Enrichment",
        "description": "Add a non-blocking feature to enrich task data with metadata derived from artifact files, providing richer context without creating hard dependencies.",
        "details": "Create a directory `src/artifacts/`. Implement modules that can detect and parse related artifact files (e.g., complexity reports). If files are found, add the derived data to the `metadata` field of the corresponding task. The entire enrichment process must not block or fail ingestion if artifacts are absent.",
        "testStrategy": "BDD: Given an artifacts directory is absent, when ingestion runs, then it succeeds and the `metadata` field on tasks remains empty. Given a valid artifact file is present, the corresponding task is enriched with the correct metadata.",
        "priority": "low",
        "dependencies": [
          27
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create enrichment orchestrator",
            "description": "Add `src/enrichment/index.ts` exporting `enrichTasks` that iterates tasks and applies pluggable enrichers without mutating the original array.",
            "dependencies": [],
            "details": "Implement a pipeline pattern with async hooks, ensuring failures are caught and logged without aborting the ingest flow.",
            "status": "pending",
            "testStrategy": "Add unit tests verifying tasks are cloned and unchanged when no enrichers run."
          },
          {
            "id": 2,
            "title": "Implement artifact readers",
            "description": "Create modules under `src/enrichment/artifacts/` (e.g., `complexity.ts`) that read optional files from `artifacts/` and append derived metadata to tasks.",
            "dependencies": [
              "35.1"
            ],
            "details": "Ensure readers gracefully handle missing files or JSON parse errors, appending warnings into a debug log collection.",
            "status": "pending",
            "testStrategy": "Add tests with temporary directories verifying enrichment occurs when files exist and that failures are swallowed."
          },
          {
            "id": 3,
            "title": "Integrate enrichment into ingest pipeline with coverage",
            "description": "Invoke `enrichTasks` from the ingest adapter and add Jest coverage confirming enrichers execute without blocking core parsing.",
            "dependencies": [
              "35.2",
              "27.3"
            ],
            "details": "Ensure the ingest adapter returns the enriched array and exposes enrichment warnings for downstream logging.",
            "status": "pending",
            "testStrategy": "Extend ingest tests to include artifact scenarios using tmp directories."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      },
      {
        "id": 36,
        "title": "Implement Observability and Secure Logging",
        "description": "Add structured logging and security-conscious defaults to improve observability and prevent accidental leakage of sensitive information.",
        "details": "Integrate a lightweight logging library. Add a `--verbose` flag to the CLI and a corresponding option for the server to emit structured logs to stderr. By default, log levels should not expose task content. Redact task IDs at `debug` level unless an `--unsafe-logs` flag is explicitly used.",
        "testStrategy": "Verify that running a command without flags produces no verbose output. Running with `--verbose` produces structured logs on stderr. Check that task titles/descriptions are not present in default logs.",
        "priority": "low",
        "dependencies": [
          31
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Introduce structured logging dependency and factory",
            "description": "Install `pino` and create `src/logging.ts` exporting `createLogger` with base configuration and serializers.",
            "dependencies": [],
            "details": "Configure default level `info`, pretty-print toggle for development, and redact task-sensitive fields using pino's `redact` option.",
            "status": "pending",
            "testStrategy": "Add unit tests verifying redaction configuration removes sensitive keys and that logger instantiation succeeds."
          },
          {
            "id": 2,
            "title": "Implement secure logging defaults and CLI flags",
            "description": "Add CLI-global `--verbose` and `--unsafe-logs` flags to control logger level and redaction, wiring them through `createLogger`.",
            "dependencies": [
              "36.1",
              "31.1"
            ],
            "details": "Propagate the logger instance to command handlers via a shared context object and document flag usage.",
            "status": "pending",
            "testStrategy": "Add CLI tests verifying verbose mode emits debug entries while default mode redacts sensitive fields."
          },
          {
            "id": 3,
            "title": "Integrate logging into MCP server",
            "description": "Inject the structured logger into the MCP server to log lifecycle events, handler calls, and errors with secure defaults.",
            "dependencies": [
              "36.1",
              "30.1"
            ],
            "details": "Ensure error logs omit task payloads unless unsafe mode is explicitly enabled; include correlation IDs for requests.",
            "status": "pending",
            "testStrategy": "Add server integration tests verifying log outputs using pino transport mocks."
          },
          {
            "id": 4,
            "title": "Document observability configuration and runbook",
            "description": "Update `docs/observability.md` (create if absent) describing logger usage, redaction defaults, and flag matrix for CLI/server.",
            "dependencies": [
              "36.2",
              "36.3"
            ],
            "details": "Include instructions for piping logs to JSON collectors and highlight security considerations when disabling redaction.",
            "status": "pending",
            "testStrategy": "Peer review documentation for accuracy; no automated test required."
          }
        ],
        "source_doc": "artifacts/delta-20250921-173948.md",
        "lineage": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ],
        "supersedes": [
          9,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-18T19:25:07.429Z",
      "updated": "2025-09-22T02:54:18.798Z",
      "description": "Tasks for master context"
    }
  }
}